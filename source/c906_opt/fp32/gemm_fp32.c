/*
 * Copyright (C) 2016-2023 T-Head Semiconductor Co., Ltd. All rights reserved.
 *
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the License); you may
 * not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an AS IS BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/* SHL version 2.1.x */

#include "shl_c906.h"

/* The matrices are stored in row-major order */
#define A(i, j) a[(i)*lda + (j)]
#define B(i, j) b[(i)*ldb + (j)]
#define C(i, j) c[(i)*ldc + (j)]

#define DECOMPOSE_K  \
    int ktmp = k;    \
    int k8 = k >> 3; \
    k -= (k8 << 3);  \
    int k4 = k >> 2; \
    k -= (k4 << 2);  \
    int k2 = k >> 1; \
    k -= (k2 << 1);  \
    int k1 = k;      \
    k = ktmp;

#define DECOMPOSE_N  \
    int ntmp = n;    \
    int n4 = n >> 2; \
    n -= (n4 << 2);  \
    int n2 = n >> 1; \
    n -= (n2 << 1);  \
    int n1 = n;      \
    n = ntmp;

#define DECOMPOSE_M  \
    int mtmp = m;    \
    int m4 = m >> 2; \
    m -= (m4 << 2);  \
    int m2 = m >> 1; \
    m -= (m2 << 1);  \
    int m1 = m;      \
    m = mtmp;

/*
    change memory layout for matrix A (kernel matrix)
    memory index from  ------>  to
    0  1  2  3                  0  4  8  12
    4  5  6  7                  1  5  9  13
    8  9  10 11                 2  6  10 14
    12 13 14 15                 3  7  11 15
    16 17 18 19                 16 18 20 22
    20 21 22 23                 17 19 21 23
    24 25 26 27                 24 25 26 27

    notice: called in the initialization function (shl_c906_conv2d_init)
*/
void shl_c906_reorder_kernel(float *a, float *sa, int m, int k, int ldx)
{
#if __riscv_vector == 128
    DECOMPOSE_M
    DECOMPOSE_K
    /*
        Execution delay cycles: vlsw + vsw  = 6 + 1
                                vlw  + vssw = 4 + 2  âœ”
    */
    if (m4 > 0) {
        float *a0 = a;
        float *a1 = a0 + ldx;
        float *a2 = a1 + ldx;
        float *a3 = a2 + ldx;
        int k_tail = k & 7;
        int store_stride = 16;
        asm volatile(
            "slli       t3, %10, 2\n\t"  // t3 = ldx * 4
            "slli       t4, t3, 2\n\t"   // t4 = 4 * ldx * 4
            "mv         t2, %5\n\t"      // t2 = m4
            "slli       t0, %7, 2\n\t"   // t0 = k_tail * 4
            "slli       t1, t0, 2\n\t"   // t1 = t0 * 4

            "1:\n\t"
            // start packm4
            "mv         %0, %9\n\t"      // a0 = a
            "add        %1, %0, t3\n\t"  // a1 = a0 + 4 * ldx
            "add        %2, %1, t3\n\t"  // a2 = a1 + 4 * ldx
            "add        %3, %2, t3\n\t"  // a3 = a2 + 4 * ldx
            "mv         t6, %6\n\t"      // t6 = k8
            "beqz       t6, 3f\n\t"      // k8 == 0 ?
            "vsetvli    zero, zero, e32, m2\n\t"

            "2:\n\t"
            // start subpack_m4k8
            "vlw.v      v0, (%0)\n\t"
            "addi       %0, %0, 32\n\t"
            "vlw.v      v2, (%1)\n\t"
            "addi       %1, %1, 32\n\t"
            "vlw.v      v4, (%2)\n\t"
            "addi       %2, %2, 32\n\t"
            "vlw.v      v6, (%3)\n\t"
            "addi       %3, %3, 32\n\t"

            "vssw.v     v0, (%4), %8\n\t"
            "addi       %4, %4, 4\n\t"
            "vssw.v     v2, (%4), %8\n\t"
            "addi       %4, %4, 4\n\t"
            "vssw.v     v4, (%4), %8\n\t"
            "addi       %4, %4, 4\n\t"
            "vssw.v     v6, (%4), %8\n\t"
            "addi       %4, %4, 116\n\t"  // sa += 32 ele * 4

            "addi       t6, t6, -1\n\t"  // k8--
            "bnez       t6, 2b\n\t"

            "3:\n\t"
            "beqz       %7, 4f\n\t"  // k_tail == 0 ?
            // Processing k_tail
            "vsetvli    zero, %7, e32, m2\n\t"
            "vlw.v      v0, (%0)\n\t"
            "add        %0, %0, t0\n\t"
            "vlw.v      v2, (%1)\n\t"
            "add        %1, %1, t0\n\t"
            "vlw.v      v4, (%2)\n\t"
            "add        %2, %2, t0\n\t"
            "vlw.v      v6, (%3)\n\t"
            "add        %3, %3, t0\n\t"

            "vssw.v     v0, (%4), %8\n\t"
            "addi       %4, %4, 4\n\t"
            "vssw.v     v2, (%4), %8\n\t"
            "addi       %4, %4, 4\n\t"
            "vssw.v     v4, (%4), %8\n\t"
            "addi       %4, %4, 4\n\t"
            "vssw.v     v6, (%4), %8\n\t"
            "addi       %4, %4, -12\n\t"
            "add        %4, %4, t1\n\t"  // sa += 4 * k_tail * 4

            "4:\n\t"
            // end packm4
            "add        %9, %9, t4\n\t"  // a += 4 * ldx * 4
            "addi       t2, t2, -1\n\t"  // m4--
            "bnez       t2, 1b\n\t"

            : "=r"(a0),            // %0
              "=r"(a1),            // %1
              "=r"(a2),            // %2
              "=r"(a3),            // %3
              "=r"(sa),            // %4
              "=r"(m4),            // %5
              "=r"(k8),            // %6
              "=r"(k_tail),        // %7
              "=r"(store_stride),  // %8
              "=r"(a),             // %9
              "=r"(ldx)            // %10
            : "0"(a0), "1"(a1), "2"(a2), "3"(a3), "4"(sa), "5"(m4), "6"(k8), "7"(k_tail),
              "8"(store_stride), "9"(a), "10"(ldx)
            : "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "t0", "t1", "t2", "t3", "t4", "t6");
    }
    if (m2 > 0) {
        float *a0 = a;
        float *a1 = a0 + ldx;
        int k8 = k >> 3;
        int k_tail = k & 7;
        int store_stride = 8;

        asm volatile(
            "slli       t2, %7, 3\n\t"  // t2 = ldx * 2 * 4
            "slli       t0, %4, 2\n\t"  // t0 = k_tail * 4
            "slli       t1, t0, 1\n\t"  // t1 = t0 * 2
            "beqz       %3, 2f\n\t"     // k8 == 0 ?
            "vsetvli    zero, zero, e32, m2\n\t"

            "1:\n\t"
            // start subpack_m2k8
            "vlw.v      v0, (%0)\n\t"
            "addi       %0, %0, 32\n\t"
            "vlw.v      v2, (%1)\n\t"
            "addi       %1, %1, 32\n\t"

            "vssw.v     v0, (%2), %5\n\t"
            "addi       %2, %2, 4\n\t"
            "vssw.v     v2, (%2), %5\n\t"
            "addi       %2, %2, -4\n\t"
            "addi       %2, %2, 64\n\t"  // sa += 16 ele * 4

            "addi       %3, %3, -1\n\t"
            "bnez       %3, 1b\n\t"

            "2:\n\t"
            "beqz       %4, 3f\n\t"  // k_tail == 0 ?
            // Processing k_tail
            "vsetvli    zero, %4, e32, m2\n\t"
            "vlw.v      v0, (%0)\n\t"
            "add        %0, %0, t0\n\t"
            "vlw.v      v2, (%1)\n\t"
            "add        %1, %1, t0\n\t"

            "vssw.v     v0, (%2), %5\n\t"
            "addi       %2, %2, 4\n\t"
            "vssw.v     v2, (%2), %5\n\t"
            "addi       %2, %2, -4\n\t"
            "add        %2, %2, t1\n\t"  // sa += k_tail * 2 * 4

            "3:\n\t"
            // end packm2
            "add        %6, %6, t2\n\t"

            : "=r"(a0),            // %0
              "=r"(a1),            // %1
              "=r"(sa),            // %2
              "=r"(k8),            // %3
              "=r"(k_tail),        // %4
              "=r"(store_stride),  // %5
              "=r"(a),             // %6
              "=r"(ldx)            // %7
            : "0"(a0), "1"(a1), "2"(sa), "3"(k8), "4"(k_tail), "5"(store_stride), "6"(a), "7"(ldx)
            : "v0", "v1", "v2", "v3", "t0", "t1", "t2");
    }
    if (m1 > 0) {
        memcpy(sa, a, sizeof(float) * ldx);
    }
#else
    int i = 0;
    for (; i + 3 < m; i += 4) {
        float *p0 = a;
        float *p1 = a + ldx;
        float *p2 = a + 2 * ldx;
        float *p3 = a + 3 * ldx;
        int j = 0;
        for (; j + 7 < k; j += 8) {
            sa[0] = p0[0];
            sa[16] = p0[4];
            sa[1] = p1[0];
            sa[17] = p1[4];
            sa[2] = p2[0];
            sa[18] = p2[4];
            sa[3] = p3[0];
            sa[19] = p3[4];

            sa[4] = p0[1];
            sa[20] = p0[5];
            sa[5] = p1[1];
            sa[21] = p1[5];
            sa[6] = p2[1];
            sa[22] = p2[5];
            sa[7] = p3[1];
            sa[23] = p3[5];

            sa[8] = p0[2];
            sa[24] = p0[6];
            sa[9] = p1[2];
            sa[25] = p1[6];
            sa[10] = p2[2];
            sa[26] = p2[6];
            sa[11] = p3[2];
            sa[27] = p3[6];

            sa[12] = p0[3];
            sa[28] = p0[7];
            sa[13] = p1[3];
            sa[29] = p1[7];
            sa[14] = p2[3];
            sa[30] = p2[7];
            sa[15] = p3[3];
            sa[31] = p3[7];

            sa += 32;
            p0 += 8;
            p1 += 8;
            p2 += 8;
            p3 += 8;
        }
        if (j + 3 < k) {
            j += 4;
            sa[0] = p0[0];
            sa[8] = p0[2];
            sa[1] = p1[0];
            sa[9] = p1[2];
            sa[2] = p2[0];
            sa[10] = p2[2];
            sa[3] = p3[0];
            sa[11] = p3[2];

            sa[4] = p0[1];
            sa[12] = p0[3];
            sa[5] = p1[1];
            sa[13] = p1[3];
            sa[6] = p2[1];
            sa[14] = p2[3];
            sa[7] = p3[1];
            sa[15] = p3[3];

            sa += 16;
            p0 += 4;
            p1 += 4;
            p2 += 4;
            p3 += 4;
        }
        if (j + 1 < k) {
            j += 2;
            sa[0] = p0[0];
            sa[1] = p1[0];
            sa[2] = p2[0];
            sa[3] = p3[0];

            sa[4] = p0[1];
            sa[5] = p1[1];
            sa[6] = p2[1];
            sa[7] = p3[1];

            sa += 8;
            p0 += 2;
            p1 += 2;
            p2 += 2;
            p3 += 2;
        }
        if (j < k) {
            sa[0] = p0[0];
            sa[1] = p1[0];
            sa[2] = p2[0];
            sa[3] = p3[0];

            sa += 4;
        }
        a += 4 * ldx;
    }
    if (i + 1 < m) {
        i += 2;
        float *p0 = a;
        float *p1 = a + ldx;

        int j = 0;
        for (; j + 7 < k; j += 8) {
            sa[0] = p0[0];
            sa[1] = p1[0];
            sa[2] = p0[1];
            sa[3] = p1[1];
            sa[4] = p0[2];
            sa[5] = p1[2];
            sa[6] = p0[3];
            sa[7] = p1[3];
            sa[8] = p0[4];
            sa[9] = p1[4];
            sa[10] = p0[5];
            sa[11] = p1[5];
            sa[12] = p0[6];
            sa[13] = p1[6];
            sa[14] = p0[7];
            sa[15] = p1[7];

            sa += 16;
            p0 += 8;
            p1 += 8;
        }
        if (j + 3 < k) {
            j += 4;
            sa[0] = p0[0];
            sa[1] = p1[0];
            sa[2] = p0[1];
            sa[3] = p1[1];
            sa[4] = p0[2];
            sa[5] = p1[2];
            sa[6] = p0[3];
            sa[7] = p1[3];

            sa += 8;
            p0 += 4;
            p1 += 4;
        }
        if (j + 1 < k) {
            j += 2;
            sa[0] = p0[0];
            sa[1] = p1[0];
            sa[2] = p0[1];
            sa[3] = p1[1];

            sa += 4;
            p0 += 2;
            p1 += 2;
        }
        if (j < k) {
            sa[0] = p0[0];
            sa[1] = p1[0];

            sa += 2;
        }
        a += 2 * ldx;
    }
    if (i < m) {
        memcpy(sa, a, sizeof(float) * ldx);
    }
#endif  // __riscv_vector
}

void shl_c906_reorder_input(float *b, float *sb, int k, int n, int ldx)
{
#if __riscv_vector == 128
    DECOMPOSE_N
    DECOMPOSE_K
    if (n4 > 0) {
        float *b0 = b;
        float *b1 = b0 + 1;
        float *b2 = b1 + 1;
        float *b3 = b2 + 1;
        int k_tail = k & 7;
        int load_stride = 4 * ldx;
        int store_stride = 16;
        asm volatile(
            "slli       t0, %11, 5\n\t"  // t0 = 8 * ldx * 4
            "slli       t1, %7, 4\n\t"   // t1 = 4 * k_tail * 4

            "1:\n\t"
            // start packn4
            "mv         %0, %10\n\t"    // b0 = b
            "addi       %1, %0, 4\n\t"  // b1 = b0 + 1
            "addi       %2, %1, 4\n\t"  // b2 = b1 + 1
            "addi       %3, %2, 4\n\t"  // b3 = b2 + 1
            "mv         t6, %6\n\t"     // t6 = k8
            "beqz       t6, 3f\n\t"     // k8 == 0 ?
            "vsetvli    zero, zero, e32, m2\n\t"

            "2:\n\t"
            // start subpack_n4k8
            "vlsw.v     v0, (%0), %8\n\t"
            "vlsw.v     v2, (%1), %8\n\t"
            "vlsw.v     v4, (%2), %8\n\t"
            "vlsw.v     v6, (%3), %8\n\t"
            "add        %0, %0, t0\n\t"
            "add        %1, %1, t0\n\t"
            "add        %2, %2, t0\n\t"
            "add        %3, %3, t0\n\t"

            "vssw.v     v0, (%4), %9\n\t"
            "addi       %4, %4, 4\n\t"
            "vssw.v     v2, (%4), %9\n\t"
            "addi       %4, %4, 4\n\t"
            "vssw.v     v4, (%4), %9\n\t"
            "addi       %4, %4, 4\n\t"
            "vssw.v     v6, (%4), %9\n\t"
            "addi       %4, %4, -12\n\t"
            "addi       %4, %4, 128\n\t"  // sb += 32 * 4

            "addi       t6, t6, -1\n\t"  // k8--
            "bnez       t6, 2b\n\t"

            "3:\n\t"
            "beqz       %7, 4f\n\t"  // k_tail == 0 ?
            // Processing k_tail
            "vsetvli    zero, %7, e32, m2\n\t"
            "vlsw.v     v0, (%0), %8\n\t"
            "vlsw.v     v2, (%1), %8\n\t"
            "vlsw.v     v4, (%2), %8\n\t"
            "vlsw.v     v6, (%3), %8\n\t"

            "vssw.v     v0, (%4), %9\n\t"
            "addi       %4, %4, 4\n\t"
            "vssw.v     v2, (%4), %9\n\t"
            "addi       %4, %4, 4\n\t"
            "vssw.v     v4, (%4), %9\n\t"
            "addi       %4, %4, 4\n\t"
            "vssw.v     v6, (%4), %9\n\t"
            "addi       %4, %4, -12\n\t"
            "add        %4, %4, t1\n\t"  // sb += k_tail * 4 * 4

            "4:\n\t"
            // end packn4
            "addi %10, %10, 16\n\t"  // b += 4 * 4
            "addi %5, %5, -1\n\t"    // n4--
            "bnez %5, 1b\n\t"

            : "=r"(b0),            // %0
              "=r"(b1),            // %1
              "=r"(b2),            // %2
              "=r"(b3),            // %3
              "=r"(sb),            // %4
              "=r"(n4),            // %5
              "=r"(k8),            // %6
              "=r"(k_tail),        // %7
              "=r"(load_stride),   // %8
              "=r"(store_stride),  // %9
              "=r"(b),             // %10
              "=r"(ldx)            // %11
            : "0"(b0), "1"(b1), "2"(b2), "3"(b3), "4"(sb), "5"(n4), "6"(k8), "7"(k_tail),
              "8"(load_stride), "9"(store_stride), "10"(b), "11"(ldx)
            : "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "t0", "t1", "t6");
    }
    int n_tail = n & 3;
    if (n_tail > 0) {
        float *b0 = b;
        int k_tail = k & 7;
        int load_stride = 4 * ldx;
        asm volatile(
            "slli       t0, %7, 5\n\t"  // t0 = 8 * ldx * 4
            "slli       t1, %4, 2\n\t"  // t1 = k_tail * 4

            "1:\n\t"
            // pack remain n_tail cols one by one
            "mv         %0, %6\n\t"  // b0 = b
            "mv         t3, %3\n\t"  // t3 = k8
            "beqz       t3, 3f\n\t"  // k8 == 0 ?
            "vsetvli    zero, zero, e32, m2\n\t"

            "2:\n\t"
            // start subpack_n1k8
            "vlsw.v     v0, (%0), %5\n\t"
            "add        %0, %0, t0\n\t"
            "vsw.v      v0, (%1)\n\t"
            "addi       %1, %1, 32\n\t"  // sb += 8 * 4

            "addi       t3, t3, -1\n\t"  // k8--
            "bnez       t3, 2b\n\t"

            "3:\n\t"
            "beqz       %4, 4f\n\t"  // k_tail == 0 ?
            // Processing k_tail
            "vsetvli    zero, %4, e32, m2\n\t"
            "vlsw.v     v0, (%0), %5\n\t"
            "vsw.v      v0, (%1)\n\t"
            "add        %1, %1, t1\n\t"

            "4:\n\t"
            // end packn1
            "addi       %6, %6, 4\n\t"  // b += 1 * 4
            "addi       %2, %2, -1\n\t"
            "bnez       %2, 1b\n\t"

            : "=r"(b0),           // %0
              "=r"(sb),           // %1
              "=r"(n_tail),       // %2
              "=r"(k8),           // %3
              "=r"(k_tail),       // %4
              "=r"(load_stride),  // %5
              "=r"(b),            // %6
              "=r"(ldx)           // %7
            : "0"(b0), "1"(sb), "2"(n_tail), "3"(k8), "4"(k_tail), "5"(load_stride), "6"(b),
              "7"(ldx)
            : "v0", "v1", "t0", "t1", "t3");
    }
#else
    int i = 0;
    for (; i + 3 < n; i += 4) {
        const float *p0 = b + i;
        const float *p1 = b + 1 * ldx + i;
        const float *p2 = b + 2 * ldx + i;
        const float *p3 = b + 3 * ldx + i;

        const float *p4 = b + 4 * ldx + i;
        const float *p5 = b + 5 * ldx + i;
        const float *p6 = b + 6 * ldx + i;
        const float *p7 = b + 7 * ldx + i;

        int j = 0;
        for (; j + 7 < k; j += 8) {
            sb[0] = p0[0];
            sb[4] = p1[0];
            sb[1] = p0[1];
            sb[5] = p1[1];
            sb[2] = p0[2];
            sb[6] = p1[2];
            sb[3] = p0[3];
            sb[7] = p1[3];

            sb[8] = p2[0];
            sb[12] = p3[0];
            sb[9] = p2[1];
            sb[13] = p3[1];
            sb[10] = p2[2];
            sb[14] = p3[2];
            sb[11] = p2[3];
            sb[15] = p3[3];

            sb[16] = p4[0];
            sb[20] = p5[0];
            sb[17] = p4[1];
            sb[21] = p5[1];
            sb[18] = p4[2];
            sb[22] = p5[2];
            sb[19] = p4[3];
            sb[23] = p5[3];

            sb[24] = p6[0];
            sb[28] = p7[0];
            sb[25] = p6[1];
            sb[29] = p7[1];
            sb[26] = p6[2];
            sb[30] = p7[2];
            sb[27] = p6[3];
            sb[31] = p7[3];

            sb += 32;
            p0 += 8 * ldx;
            p1 += 8 * ldx;
            p2 += 8 * ldx;
            p3 += 8 * ldx;
            p4 += 8 * ldx;
            p5 += 8 * ldx;
            p6 += 8 * ldx;
            p7 += 8 * ldx;
        }
        if (j + 3 < k) {
            j += 4;
            sb[0] = p0[0];
            sb[1] = p0[1];
            sb[2] = p0[2];
            sb[3] = p0[3];

            sb[4] = p1[0];
            sb[5] = p1[1];
            sb[6] = p1[2];
            sb[7] = p1[3];

            sb[8] = p2[0];
            sb[9] = p2[1];
            sb[10] = p2[2];
            sb[11] = p2[3];

            sb[12] = p3[0];
            sb[13] = p3[1];
            sb[14] = p3[2];
            sb[15] = p3[3];

            sb += 16;
            p0 += 4 * ldx;
            p1 += 4 * ldx;
            p2 += 4 * ldx;
            p3 += 4 * ldx;
        }
        if (j + 1 < k) {
            j += 2;
            sb[0] = p0[0];
            sb[1] = p0[1];
            sb[2] = p0[2];
            sb[3] = p0[3];

            sb[4] = p1[0];
            sb[5] = p1[1];
            sb[6] = p1[2];
            sb[7] = p1[3];

            sb += 8;
            p0 += 2 * ldx;
            p1 += 2 * ldx;
        }
        if (j < k) {
            sb[0] = p0[0];
            sb[1] = p0[1];
            sb[2] = p0[2];
            sb[3] = p0[3];

            sb += 4;
            p0 += ldx;
        }
    }
    while (i < n) {
        const float *p = b + i;
        for (int j = 0; j < k; j++) {
            *sb = *p;
            sb++;
            p += ldx;
        }
        i++;
    }

#endif  // __riscv_vector
}

void shl_c906_reorder_input_1(float *b, float *sb, int k, int n, int ldx)
{
    asm volatile(
        "vsetvli        zero, zero, e32, m1\n\t"  // set vl = 8

        "slli           t2, %4, 2\n\t"  // t2 = ldx * 4 (line stride)

        "srai           t0, %3, 2\n\t"  // t0 = n4
        "beqz           t0, 3f\n\t"     // jump to packn_tail

        "1:\n\t"  // n4
        "mv             a0, %0\n\t"
        "addi           %0, %0, 16\n\t"
        "mv             t1, %2\n\t"  // k

        "2:\n\t"
        // start packn8k1
        "vle.v          v2, (a0)\n\t"
        "add            a0, a0, t2\n\t"
        "vse.v          v2, (%1)\n\t"
        "addi           %1, %1, 16\n\t"

        "addi           t1, t1, -1\n\t"
        "bnez           t1, 2b\n\t"

        "addi           t0, t0, -1\n\t"
        "bnez           t0, 1b\n\t"

        "3:\n\t"                        // n_tail
        "andi           t0, %3, 3\n\t"  // n & 3u
        "beqz           t0, 8f\n\t"

        "srai           t3, %2, 2\n\t"  // k4
        "slli           t5, %4, 4\n\t"  // t5 = ldx * 4 * 4 (4 lines)
        "andi           t6, %2, 3\n\t"  // k_tail
        "slli           t4, t6, 2\n\t"  // k_tail * 4

        "4:\n\t"
        "mv             a0, %0\n\t"
        "addi           %0, %0, 4\n\t"
        "mv             t1, t3\n\t"  // t1 = k4
        "beqz           t3, 6f\n\t"

        "5:\n\t"
        "vsetvli        zero, zero, e32, m1\n\t"
        "vlse.v         v2, (a0), t2\n\t"
        "add            a0, a0, t5\n\t"
        "vse.v          v2, (%1)\n\t"
        "addi           %1, %1, 16\n\t"

        "addi           t1, t1, -1\n\t"
        "bnez           t1, 5b\n\t"

        "6:\n\t"
        "vsetvli        zero, t6, e32, m1\n\t"
        "vlse.v         v2, (a0), t2\n\t"
        "vse.v          v2, (%1)\n\t"
        "add            %1, %1, t4\n\t"

        "7:\n\t"
        "addi           t0, t0, -1\n\t"
        "bnez           t0, 4b\n\t"

        "8:\n\t"  // ending

        : "=r"(b),   // %0
          "=r"(sb),  // %1
          "=r"(k),   // %2
          "=r"(n),   // %3
          "=r"(ldx)  // %4
        : "0"(b), "1"(sb), "2"(k), "3"(n), "4"(ldx)
        : "v0", "v2", "a0", "t0", "t1", "t2", "t3", "t4", "t5", "t6");
}

static inline void kernel_m1_f32(float *dst, float *sa, float *sb, int m, int k, int n, int ldc,
                                 float *bias, bool fuse_relu)
{
    float *pa = sa;
    float *pb = sb;
    float *pc = dst;
    DECOMPOSE_K
    DECOMPOSE_N

#if __riscv_vector == 128
    if (n4 > 0) {
        asm volatile(
            "vsetvli    zero, zero, e32, m1\n\t"
            "flw        ft0, (%8)\n\t"  // bias

            "beqz       %9, 1f\n\t"    // if fuse_relu == 0
            "vmv.v.x    v0, zero\n\t"  // v0 hold const zero, using for relu

            "1:\n\t"
            // start kernel_m1n4
            "vfmv.v.f   v24, ft0\n\t"  // v24[0..3] = *bias
            // "vlw.v      v24, (%8)\n\t"      // v24[0..3] = bias[0..3]
            // "addi       %8, %8, 16\n\t"

            "mv         a1, %0\n\t"  // a1 = pa
            "mv         t0, %3\n\t"  // t0 = k8
            "beqz       t0, 3f\n\t"  // k8 == 0 ?

            "2:\n\t"
            // start subkernel_m1n4k8
            "vlw.v      v1, (%1)\n\t"    // load pb
            "flw        ft1, 0(a1)\n\t"  // load pa
            "vfmv.v.f   v2, ft1\n\t"
            "addi       %1, %1, 16\n\t"   // pb += 4 * 4
            "vfmacc.vv  v24, v1, v2\n\t"  // 0

            "vlw.v      v3, (%1)\n\t"
            "flw        ft2, 4(a1)\n\t"
            "vfmv.v.f   v4, ft2\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v3, v4\n\t"  // 1

            "vlw.v      v5, (%1)\n\t"
            "flw        ft3, 8(a1)\n\t"
            "vfmv.v.f   v6, ft3\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v5, v6\n\t"  // 2

            "vlw.v      v7, (%1)\n\t"
            "flw        ft4, 12(a1)\n\t"
            "vfmv.v.f   v8, ft4\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v7, v8\n\t"  // 3

            "vlw.v      v9, (%1)\n\t"
            "flw        ft5, 16(a1)\n\t"
            "vfmv.v.f   v10, ft5\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v9, v10\n\t"  // 4

            "vlw.v      v11, (%1)\n\t"
            "flw        ft6, 20(a1)\n\t"
            "vfmv.v.f   v12, ft6\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v11, v12\n\t"  // 5

            "vlw.v      v13, (%1)\n\t"
            "flw        ft7, 24(a1)\n\t"
            "vfmv.v.f   v14, ft7\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v13, v14\n\t"  // 6

            "vlw.v      v15, (%1)\n\t"
            "flw        ft8, 28(a1)\n\t"
            "vfmv.v.f   v16, ft8\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v15, v16\n\t"  // 7
            "addi       a1, a1, 32\n\t"

            "addi       t0, t0, -1\n\t"
            "bnez       t0, 2b\n\t"

            "3:\n\t"
            "beqz       %4, 4f\n\t"  // k4 == 0 ?
            // start subkernel_m1n4k4
            "vlw.v      v1, (%1)\n\t"
            "flw        ft1, 0(a1)\n\t"
            "vfmv.v.f   v2, ft1\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v1, v2\n\t"  // 0

            "vlw.v      v3, (%1)\n\t"
            "flw        ft2, 4(a1)\n\t"
            "vfmv.v.f   v4, ft2\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v3, v4\n\t"  // 1

            "vlw.v      v5, (%1)\n\t"
            "flw        ft3, 8(a1)\n\t"
            "vfmv.v.f   v6, ft3\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v5, v6\n\t"  // 2

            "vlw.v      v7, (%1)\n\t"
            "flw        ft4, 12(a1)\n\t"
            "vfmv.v.f   v8, ft4\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v7, v8\n\t"  // 3
            "addi       a1, a1, 16\n\t"

            "4:\n\t"
            "beqz       %5, 5f\n\t"  // k2 == 0 ?
            // start subkernel_m1n4k2
            "vlw.v      v1, (%1)\n\t"
            "flw        ft1, 0(a1)\n\t"
            "vfmv.v.f   v2, ft1\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v1, v2\n\t"  // 0

            "vlw.v      v3, (%1)\n\t"
            "flw        ft2, 4(a1)\n\t"
            "vfmv.v.f   v4, ft2\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v3, v4\n\t"  // 1
            "addi       a1, a1, 8\n\t"

            "5:\n\t"
            "beqz       %6, 6f\n\t"  // k1 == 0 ?
            // start subkernel_m1n4k1
            "vlw.v      v1, (%1)\n\t"
            "flw        ft1, 0(a1)\n\t"
            "vfmv.v.f   v2, ft1\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v1, v2\n\t"  // 0
            "addi       a1, a1, 4\n\t"

            "6:\n\t"
            "beqz       %9, 7f\n\t"
            // fused relu
            "vfmax.vv   v24, v24, v0\n\t"  // **** relu ****

            "7:\n\t"
            // end kernel_m1n4
            "vsw.v      v24, (%2)\n\t"
            "addi       %2, %2, 16\n\t"  // pc += 4 * 4

            "addi       %7, %7, -1\n\t"
            "bnez       %7, 1b\n\t"

            : "=r"(pa),        // %0
              "=r"(pb),        // %1
              "=r"(pc),        // %2
              "=r"(k8),        // %3
              "=r"(k4),        // %4
              "=r"(k2),        // %5
              "=r"(k1),        // %6
              "=r"(n4),        // %7
              "=r"(bias),      // %8
              "=r"(fuse_relu)  // %9
            : "0"(pa), "1"(pb), "2"(pc), "3"(k8), "4"(k4), "5"(k2), "6"(k1), "7"(n4), "8"(bias),
              "9"(fuse_relu)
            : "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8", "v9", "v10", "v11", "v12",
              "v13", "v14", "v15", "v16", "v24", "a1", "t0", "ft0", "ft1", "ft2", "ft3", "ft4",
              "ft5", "ft6", "ft7", "ft8");
    }
    if (n2 > 0) {
        int k_tail = k & 7;
        float *pb0 = pb;
        float *pb1 = pb0 + k;

        asm volatile(
            "fmv.w.x        ft4, zero\n\t"  // for fuse relu
            "mv             t4, %4\n\t"     // t4 = k8
            "vsetvli        zero, zero, e32, m2\n\t"
            "vxor.vv        v6, v6, v6\n\t"  // clear
            "vxor.vv        v8, v8, v8\n\t"  // clear
            "flw            ft0, 0(%6)\n\t"  // ft0 = *bias
            // "flw            ft3, 4(%6)\n\t"         // ft3 = *(bias + 1)
            // "addi           %6, %6, 8\n\t"
            "vfmv.s.f       v10, ft0\n\t"  // v10[0] = ft0
            "vfmv.s.f       v12, ft0\n\t"  // v10[0] = ft0
            // "vfmv.s.f       v12, ft3\n\t"           // v12[0] = ft3

            "beqz           %5, 1f\n\t"  // k_tail == 0 ?
            // Processing k_tail
            "slli           t0, %5, 2\n\t"  // t0 = k_tail * 4
            "vsetvli        zero, %5, e32, m2\n\t"
            "vlw.v          v0, (%0)\n\t"
            "add            %0, %0, t0\n\t"
            "vlw.v          v2, (%1)\n\t"
            "add            %1, %1, t0\n\t"
            "vlw.v          v4, (%2)\n\t"
            "add            %2, %2, t0\n\t"
            "vfmacc.vv      v6, v0, v2\n\t"
            "vfmacc.vv      v8, v0, v4\n\t"
            "beqz           t4, 2f\n\t"  // k8 == 0 ?
            "vsetvli        zero, zero, e32, m2\n\t"

            "1:\n\t"
            // start subkernel_m1n2k8
            "vlw.v          v0, (%0)\n\t"
            "addi           %0, %0, 32\n\t"
            "vlw.v          v2, (%1)\n\t"
            "addi           %1, %1, 32\n\t"
            "vlw.v          v4, (%2)\n\t"
            "addi           %2, %2, 32\n\t"
            "vfmacc.vv      v6, v0, v2\n\t"
            "vfmacc.vv      v8, v0, v4\n\t"
            "addi           t4, t4, -1\n\t"
            "bnez           t4, 1b\n\t"

            "2:\n\t"
            // end kernel_m1n2
            "vfredsum.vs    v10, v6, v10\n\t"  // v10[0] = v10[0] + sum(v6[0..i])
            "vfredsum.vs    v12, v8, v12\n\t"  // v12[0] = v12[0] + sum(v8[0..i])
            "vfmv.f.s       ft1, v10\n\t"
            "vfmv.f.s       ft2, v12\n\t"

            "beqz           %7, 3f\n\t"
            // fuse relu
            "fmax.s         ft1, ft1, ft4\n\t"  // **** relu ****
            "fmax.s         ft2, ft2, ft4\n\t"  // **** relu ****

            "3:\n\t"

            "fsw            ft1, 0(%3)\n\t"
            "fsw            ft2, 4(%3)\n\t"

            : "=r"(pa),        // %0
              "=r"(pb0),       // %1
              "=r"(pb1),       // %2
              "=r"(pc),        // %3
              "=r"(k8),        // %4
              "=r"(k_tail),    // %5
              "=r"(bias),      // %6
              "=r"(fuse_relu)  // %7
            : "0"(pa), "1"(pb0), "2"(pb1), "3"(pc), "4"(k8), "5"(k_tail), "6"(bias), "7"(fuse_relu)
            : "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8", "v9", "v10", "v11", "v12",
              "v13", "ft0", "ft1", "ft2", "ft3", "ft4", "t0", "t4");
        pb += 2 * k;
        pc += 2;
    }
    if (n1 > 0) {
        pa = sa;
        int k_tail = k & 7;
        asm volatile(
            "fmv.w.x        ft2, zero\n\t"  // for fuse relu
            "vsetvli        zero, zero, e32, m2\n\t"
            "vxor.vv        v4, v4, v4\n\t"  // clear

            "flw            ft0, 0(%5)\n\t"  // ft0 = *bias
            "vfmv.s.f       v6, ft0\n\t"     // v6[0] = ft0

            "beqz           %4, 1f\n\t"  // k_tail == 0 ?
            // Processing k_tail
            "slli           t0, %4, 2\n\t"  // t0 = k_tail * 4
            "vsetvli        zero, %4, e32, m2\n\t"
            "vlw.v          v0, (%0)\n\t"
            "add            %0, %0, t0\n\t"
            "vlw.v          v2, (%1)\n\t"
            "add            %1, %1, t0\n\t"
            "vfmacc.vv      v4, v0, v2\n\t"
            "beqz           %3, 2f\n\t"  // k8 == 0 ?
            "vsetvli        zero, zero, e32, m2\n\t"

            "1:\n\t"
            // start subkernel_m1n1k8
            "vlw.v          v0, (%0)\n\t"
            "addi           %0, %0, 32\n\t"
            "vlw.v          v2, (%1)\n\t"
            "addi           %1, %1, 32\n\t"
            "vfmacc.vv      v4, v0, v2\n\t"
            "addi           %3, %3, -1\n\t"
            "bnez           %3, 1b\n\t"

            "2:\n\t"
            // end kernel_m1n1
            "vfredsum.vs    v6, v4, v6\n\t"  // v6[0] = v6[0] + sum(v4[0..i])
            "vfmv.f.s       ft1, v6\n\t"

            "beqz           %6, 3f\n\t"
            // fused relu
            "fmax.s         ft1, ft1, ft2\n\t"  // **** relu ****

            "3:\n\t"
            "fsw            ft1, 0(%2)\n\t"

            : "=r"(pa),        // %0
              "=r"(pb),        // %1
              "=r"(pc),        // %2
              "=r"(k8),        // %3
              "=r"(k_tail),    // %4
              "=r"(bias),      // %5
              "=r"(fuse_relu)  // %6
            : "0"(pa), "1"(pb), "2"(pc), "3"(k8), "4"(k_tail), "5"(bias), "6"(fuse_relu)
            : "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "ft0", "ft1", "ft2", "t0");
    }
#else
    for (int i = 0; i < n4; i++) {
        int j = 0;
        pa = sa;
        pc[0] = pc[1] = pc[2] = pc[3] = *bias;
        for (; j + 7 < k; j += 8) {
            pc[0] += pa[0] * pb[0];
            pc[1] += pa[0] * pb[1];
            pc[2] += pa[0] * pb[2];
            pc[3] += pa[0] * pb[3];

            pc[0] += pa[1] * pb[4];
            pc[1] += pa[1] * pb[5];
            pc[2] += pa[1] * pb[6];
            pc[3] += pa[1] * pb[7];

            pc[0] += pa[2] * pb[8];
            pc[1] += pa[2] * pb[9];
            pc[2] += pa[2] * pb[10];
            pc[3] += pa[2] * pb[11];

            pc[0] += pa[3] * pb[12];
            pc[1] += pa[3] * pb[13];
            pc[2] += pa[3] * pb[14];
            pc[3] += pa[3] * pb[15];

            pc[0] += pa[4] * pb[16];
            pc[1] += pa[4] * pb[17];
            pc[2] += pa[4] * pb[18];
            pc[3] += pa[4] * pb[19];

            pc[0] += pa[5] * pb[20];
            pc[1] += pa[5] * pb[21];
            pc[2] += pa[5] * pb[22];
            pc[3] += pa[5] * pb[23];

            pc[0] += pa[6] * pb[24];
            pc[1] += pa[6] * pb[25];
            pc[2] += pa[6] * pb[26];
            pc[3] += pa[6] * pb[27];

            pc[0] += pa[7] * pb[28];
            pc[1] += pa[7] * pb[29];
            pc[2] += pa[7] * pb[30];
            pc[3] += pa[7] * pb[31];

            pa += 8;
            pb += 32;
        }
        if (j + 3 < k) {
            j += 4;
            pc[0] += pa[0] * pb[0];
            pc[1] += pa[0] * pb[1];
            pc[2] += pa[0] * pb[2];
            pc[3] += pa[0] * pb[3];

            pc[0] += pa[1] * pb[4];
            pc[1] += pa[1] * pb[5];
            pc[2] += pa[1] * pb[6];
            pc[3] += pa[1] * pb[7];

            pc[0] += pa[2] * pb[8];
            pc[1] += pa[2] * pb[9];
            pc[2] += pa[2] * pb[10];
            pc[3] += pa[2] * pb[11];

            pc[0] += pa[3] * pb[12];
            pc[1] += pa[3] * pb[13];
            pc[2] += pa[3] * pb[14];
            pc[3] += pa[3] * pb[15];

            pa += 4;
            pb += 16;
        }
        if (j + 1 < k) {
            j += 2;
            pc[0] += pa[0] * pb[0];
            pc[1] += pa[0] * pb[1];
            pc[2] += pa[0] * pb[2];
            pc[3] += pa[0] * pb[3];

            pc[0] += pa[1] * pb[4];
            pc[1] += pa[1] * pb[5];
            pc[2] += pa[1] * pb[6];
            pc[3] += pa[1] * pb[7];

            pa += 2;
            pb += 8;
        }
        if (j < k) {
            pc[0] += pa[0] * pb[0];
            pc[1] += pa[0] * pb[1];
            pc[2] += pa[0] * pb[2];
            pc[3] += pa[0] * pb[3];

            pa += 1;
            pb += 4;
        }
        if (fuse_relu) {
            pc[0] = pc[0] > 0 ? pc[0] : 0;
            pc[1] = pc[1] > 0 ? pc[1] : 0;
            pc[2] = pc[2] > 0 ? pc[2] : 0;
            pc[3] = pc[3] > 0 ? pc[3] : 0;
        }
        pc += 4;
    }
    if (n2 > 0) {
        pa = sa;
        pc[0] = pc[1] = *bias;
        float *pb0 = pb;
        float *pb1 = pb0 + k;
        int j = 0;
        for (; j + 7 < k; j += 8) {
            pc[0] += pa[0] * pb0[0];
            pc[1] += pa[0] * pb1[0];

            pc[0] += pa[1] * pb0[1];
            pc[1] += pa[1] * pb1[1];

            pc[0] += pa[2] * pb0[2];
            pc[1] += pa[2] * pb1[2];

            pc[0] += pa[3] * pb0[3];
            pc[1] += pa[3] * pb1[3];

            pc[0] += pa[4] * pb0[4];
            pc[1] += pa[4] * pb1[4];

            pc[0] += pa[5] * pb0[5];
            pc[1] += pa[5] * pb1[5];

            pc[0] += pa[6] * pb0[6];
            pc[1] += pa[6] * pb1[6];

            pc[0] += pa[7] * pb0[7];
            pc[1] += pa[7] * pb1[7];

            pa += 8;
            pb0 += 8;
            pb1 += 8;
        }
        if (j + 3 < k) {
            j += 4;
            pc[0] += pa[0] * pb0[0];
            pc[1] += pa[0] * pb1[0];

            pc[0] += pa[1] * pb0[1];
            pc[1] += pa[1] * pb1[1];

            pc[0] += pa[2] * pb0[2];
            pc[1] += pa[2] * pb1[2];

            pc[0] += pa[3] * pb0[3];
            pc[1] += pa[3] * pb1[3];

            pa += 4;
            pb0 += 4;
            pb1 += 4;
        }
        if (j + 1 < k) {
            j += 2;
            pc[0] += pa[0] * pb0[0];
            pc[1] += pa[0] * pb1[0];

            pc[0] += pa[1] * pb0[1];
            pc[1] += pa[1] * pb1[1];

            pa += 2;
            pb0 += 2;
            pb1 += 2;
        }
        if (j < k) {
            pc[0] += pa[0] * pb0[0];
            pc[1] += pa[0] * pb1[0];

            pa += 1;
            pb0 += 1;
            pb1 += 1;
        }
        if (fuse_relu) {
            pc[0] = pc[0] > 0 ? pc[0] : 0;
            pc[1] = pc[1] > 0 ? pc[1] : 0;
        }
        pc += 2;
        pb += 2 * k;
    }
    if (n1 > 0) {
        pa = sa;
        pc[0] = *bias;
        int j = 0;
        for (; j + 7 < k; j += 8) {
            pc[0] += pa[0] * pb[0];
            pc[0] += pa[1] * pb[1];
            pc[0] += pa[2] * pb[2];
            pc[0] += pa[3] * pb[3];
            pc[0] += pa[4] * pb[4];
            pc[0] += pa[5] * pb[5];
            pc[0] += pa[6] * pb[6];
            pc[0] += pa[7] * pb[7];

            pa += 8;
            pb += 8;
        }
        if (j + 3 < k) {
            j += 4;
            pc[0] += pa[0] * pb[0];
            pc[0] += pa[1] * pb[1];
            pc[0] += pa[2] * pb[2];
            pc[0] += pa[3] * pb[3];

            pa += 4;
            pb += 4;
        }
        if (j + 1 < k) {
            j += 2;
            pc[0] += pa[0] * pb[0];
            pc[0] += pa[1] * pb[1];

            pa += 2;
            pb += 2;
        }
        if (j < k) {
            pc[0] += pa[0] * pb[0];

            pa += 1;
            pb += 1;
        }
        if (fuse_relu) {
            pc[0] = pc[0] > 0 ? pc[0] : 0;
        }
        pc += 1;
    }
#endif  // __riscv_vector
}

static inline void kernel_m2_f32(float *dst, float *sa, float *sb, int m, int k, int n, int ldc,
                                 float *bias, bool fuse_relu)
{
    float *pa = sa;
    float *pb = sb;
    float *pc0 = dst;
    float *pc1 = pc0 + ldc;
    DECOMPOSE_K
    DECOMPOSE_N
#if __riscv_vector == 128
    if (n4 > 0) {
        asm volatile(
            "vsetvli    zero, zero, e32, m1\n\t"
            "flw        ft0, (%9)\n\t"    // ft0 = *bias
            "flw        ft10, 4(%9)\n\t"  // ft1 = *(bias + 1)

            "beqz       %10, 1f\n\t"   // if fuse_relu == 0
            "vmv.v.x    v0, zero\n\t"  // v0 hold const zero, using for relu

            "1:\n\t"  // n4
            // start kernel_m2n4
            "vfmv.v.f   v24, ft0\n\t"   // v24[0..3] = ft0 = *bias
            "vfmv.v.f   v25, ft10\n\t"  // v25[0..3] = ft10 = *(bias + 1)
            // "vlw.v      v24, (%9)\n\t"          // v24[0..3] = bias[0..3]
            // "vlw.v      v25, (%9)\n\t"          // v24[0..3] = bias[0..3]
            // "addi       %9, %9, 16\n\t"

            "mv         a1, %0\n\t"  // a1 = pa
            "mv         t0, %4\n\t"  // t0 = k8
            "beqz       t0, 3f\n\t"  // k8 == 0 ?

            "2:\n\t"
            // start subkernel_m2n4k8
            "vlw.v      v1, (%1)\n\t"
            "flw        ft1, 0(a1)\n\t"
            "vfmv.v.f   v2, ft1\n\t"
            "flw        fa1, 4(a1)\n\t"
            "vfmv.v.f   v3, fa1\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v1, v2\n\t"  // 0
            "vfmacc.vv  v25, v1, v3\n\t"

            "vlw.v      v4, (%1)\n\t"
            "flw        ft2, 8(a1)\n\t"
            "vfmv.v.f   v5, ft2\n\t"
            "flw        fa2, 12(a1)\n\t"
            "vfmv.v.f   v6, fa2\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v4, v5\n\t"  // 1
            "vfmacc.vv  v25, v4, v6\n\t"

            "vlw.v      v7, (%1)\n\t"
            "flw        ft3, 16(a1)\n\t"
            "vfmv.v.f   v8, ft3\n\t"
            "flw        fa3, 20(a1)\n\t"
            "vfmv.v.f   v9, fa3\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v7, v8\n\t"  // 2
            "vfmacc.vv  v25, v7, v9\n\t"

            "vlw.v      v10, (%1)\n\t"
            "flw        ft4, 24(a1)\n\t"
            "vfmv.v.f   v11, ft4\n\t"
            "flw        fa4, 28(a1)\n\t"
            "vfmv.v.f   v12, fa4\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v10, v11\n\t"  // 3
            "vfmacc.vv  v25, v10, v12\n\t"

            "vlw.v      v13, (%1)\n\t"
            "flw        ft5, 32(a1)\n\t"
            "vfmv.v.f   v14, ft5\n\t"
            "flw        fa5, 36(a1)\n\t"
            "vfmv.v.f   v15, fa5\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v13, v14\n\t"  // 4
            "vfmacc.vv  v25, v13, v15\n\t"

            "vlw.v      v16, (%1)\n\t"
            "flw        ft6, 40(a1)\n\t"
            "vfmv.v.f   v17, ft6\n\t"
            "flw        fa6, 44(a1)\n\t"
            "vfmv.v.f   v18, fa6\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v16, v17\n\t"  // 5
            "vfmacc.vv  v25, v16, v18\n\t"

            "vlw.v      v19, (%1)\n\t"
            "flw        ft7, 48(a1)\n\t"
            "vfmv.v.f   v20, ft7\n\t"
            "flw        fa7, 52(a1)\n\t"
            "vfmv.v.f   v21, fa7\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v19, v20\n\t"  // 6
            "vfmacc.vv  v25, v19, v21\n\t"

            "vlw.v      v28, (%1)\n\t"
            "flw        ft8, 56(a1)\n\t"
            "vfmv.v.f   v29, ft8\n\t"
            "flw        fa0, 60(a1)\n\t"
            "vfmv.v.f   v30, fa0\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v28, v29\n\t"  // 7
            "vfmacc.vv  v25, v28, v30\n\t"
            "addi       a1, a1, 64\n\t"

            "addi       t0, t0, -1\n\t"
            "bnez       t0, 2b\n\t"

            "3:\n\t"
            "beqz       %5, 4f\n\t"  // k4 == 0 ?
            // start subkernel_m2n4k4
            "vlw.v      v1, (%1)\n\t"
            "flw        ft1, 0(a1)\n\t"
            "vfmv.v.f   v2, ft1\n\t"
            "flw        fa1, 4(a1)\n\t"
            "vfmv.v.f   v3, fa1\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v1, v2\n\t"  // 0
            "vfmacc.vv  v25, v1, v3\n\t"

            "vlw.v      v4, (%1)\n\t"
            "flw        ft2, 8(a1)\n\t"
            "vfmv.v.f   v5, ft2\n\t"
            "flw        fa2, 12(a1)\n\t"
            "vfmv.v.f   v6, fa2\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v4, v5\n\t"  // 1
            "vfmacc.vv  v25, v4, v6\n\t"

            "vlw.v      v7, (%1)\n\t"
            "flw        ft3, 16(a1)\n\t"
            "vfmv.v.f   v8, ft3\n\t"
            "flw        fa3, 20(a1)\n\t"
            "vfmv.v.f   v9, fa3\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v7, v8\n\t"  // 2
            "vfmacc.vv  v25, v7, v9\n\t"

            "vlw.v      v10, (%1)\n\t"
            "flw        ft4, 24(a1)\n\t"
            "vfmv.v.f   v11, ft4\n\t"
            "flw        fa4, 28(a1)\n\t"
            "vfmv.v.f   v12, fa4\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v10, v11\n\t"  // 3
            "vfmacc.vv  v25, v10, v12\n\t"
            "addi       a1, a1, 32\n\t"

            "4:\n\t"
            "beqz       %6, 5f\n\t"  // k2 == 0 ?
            // start subkernel_m2n4k2
            "vlw.v      v1, (%1)\n\t"
            "flw        ft1, 0(a1)\n\t"
            "vfmv.v.f   v2, ft1\n\t"
            "flw        fa1, 4(a1)\n\t"
            "vfmv.v.f   v3, fa1\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v1, v2\n\t"  // 0
            "vfmacc.vv  v25, v1, v3\n\t"

            "vlw.v      v4, (%1)\n\t"
            "flw        ft2, 8(a1)\n\t"
            "vfmv.v.f   v5, ft2\n\t"
            "flw        fa2, 12(a1)\n\t"
            "vfmv.v.f   v6, fa2\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v4, v5\n\t"  // 1
            "vfmacc.vv  v25, v4, v6\n\t"
            "addi       a1, a1, 16\n\t"

            "5:\n\t"
            "beqz       %7, 6f\n\t"  // k1 == 0 ?
            // start subkernel_m2n4k1
            "vlw.v      v1, (%1)\n\t"
            "flw        ft1, 0(a1)\n\t"
            "vfmv.v.f   v2, ft1\n\t"
            "flw        fa1, 4(a1)\n\t"
            "vfmv.v.f   v3, fa1\n\t"
            "addi       %1, %1, 16\n\t"
            "vfmacc.vv  v24, v1, v2\n\t"  // 0
            "vfmacc.vv  v25, v1, v3\n\t"
            "addi       a1, a1, 8\n\t"

            "6:\n\t"
            "beqz       %10, 7f\n\t"
            // fused relu
            "vfmax.vv   v25, v25, v0\n\t"  // **** relu ****
            "vfmax.vv   v25, v25, v0\n\t"  // **** relu ****

            "7:\n\t"
            // end kernel_m2n4
            "vsw.v      v24, (%2)\n\t"  // pc0[0..3] = v24
            "addi       %2, %2, 16\n\t"
            "vsw.v      v25, (%3)\n\t"  // pc1[0..3] = v25
            "addi       %3, %3, 16\n\t"

            "addi       %8, %8, -1\n\t"
            "bnez       %8, 1b\n\t"

            : "=r"(pa),        // %0
              "=r"(pb),        // %1
              "=r"(pc0),       // %2
              "=r"(pc1),       // %3
              "=r"(k8),        // %4
              "=r"(k4),        // %5
              "=r"(k2),        // %6
              "=r"(k1),        // %7
              "=r"(n4),        // %8
              "=r"(bias),      // %9
              "=r"(fuse_relu)  // %10
            : "0"(pa), "1"(pb), "2"(pc0), "3"(pc1), "4"(k8), "5"(k4), "6"(k2), "7"(k1), "8"(n4),
              "9"(bias), "10"(fuse_relu)
            : "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8", "v9", "v10", "v11", "v12", "v13",
              "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v24", "v25", "v28", "v29",
              "v30", "a1", "t0", "ft0", "ft1", "ft2", "ft3", "ft4", "ft5", "ft6", "ft7", "ft8",
              "ft9", "ft10", "fa0", "fa1", "fa2", "fa3", "fa4", "fa5", "fa6", "fa7");
    }
    if (n2 > 0) {
        int k_tail = k & 7;
        float *pa0 = sa;
        float *pa1 = pa0 + 1;
        float *pb0 = pb;
        float *pb1 = pb0 + k;
        int load_stride = 8;

        asm volatile(
            "fmv.w.x        ft6, zero\n\t"  // for fuse relu
            "mv             t6, %6\n\t"     // t6 = k8
            "vsetvli        zero, zero, e32, m2\n\t"
            "vxor.vv        v8, v8, v8\n\t"     // clear
            "vxor.vv        v10, v10, v10\n\t"  // clear
            "vxor.vv        v12, v12, v12\n\t"  // clear
            "vxor.vv        v14, v14, v14\n\t"  // clear
            "flw            ft0, 0(%8)\n\t"     // ft0 = *bias
            "flw            ft1, 4(%8)\n\t"     // ft1 = *(bias + 1)
            // "addi           %8, %8, 8\n\t"
            "vfmv.s.f       v16, ft0\n\t"  // v16[0] = ft0
            "vfmv.s.f       v18, ft0\n\t"  // v18[0] = ft0
            "vfmv.s.f       v20, ft1\n\t"  // v20[0] = ft1
            "vfmv.s.f       v22, ft1\n\t"  // v22[1] = ft1

            "beqz           %7, 1f\n\t"  // k_tail == 0 ?
            // Processing k_tail
            "slli           t0, %7, 2\n\t"  // t0 = k_tail * 4
            "slli           t1, t0, 1\n\t"  // t1 = t0 * 2
            "vsetvli        zero, %7, e32, m2\n\t"
            "vlsw.v         v0, (%0), %9\n\t"
            "add            %0, %0, t1\n\t"
            "vlsw.v         v2, (%1), %9\n\t"
            "addi           %1, %0, 4\n\t"

            "vlw.v          v4, (%2)\n\t"
            "add            %2, %2, t0\n\t"
            "vlw.v          v6, (%3)\n\t"
            "add            %3, %3, t0\n\t"

            "vfmacc.vv      v8,  v0, v4\n\t"
            "vfmacc.vv      v10, v0, v6\n\t"
            "vfmacc.vv      v12, v2, v4\n\t"
            "vfmacc.vv      v14, v2, v6\n\t"
            "beqz           t6, 2f\n\t"  // k8 == 0 ?
            "vsetvli        zero, zero, e32, m2\n\t"

            "1:\n\t"
            // start subkernel_m2n2k8
            "vlsw.v         v0, (%0), %9\n\t"
            "addi           %0, %0, 64\n\t"
            "vlsw.v         v2, (%1), %9\n\t"
            "addi           %1, %0, 4\n\t"

            "vlw.v          v4, (%2)\n\t"
            "addi           %2, %2, 32\n\t"
            "vlw.v          v6, (%3)\n\t"
            "addi           %3, %3, 32\n\t"

            "vfmacc.vv      v8,  v0, v4\n\t"
            "vfmacc.vv      v10, v0, v6\n\t"
            "vfmacc.vv      v12, v2, v4\n\t"
            "vfmacc.vv      v14, v2, v6\n\t"
            "addi           t6, t6, -1\n\t"
            "bnez           t6, 1b\n\t"

            "2:\n\t"
            // end kernel_m2n2
            "vfredsum.vs    v16, v8, v16\n\t"   // v16[0] = v16[0] + sum(v8[0..i])
            "vfredsum.vs    v18, v10, v18\n\t"  // v18[0] = v18[0] + sum(v10[0..i])
            "vfredsum.vs    v20, v12, v20\n\t"  // v20[0] = v20[0] + sum(v12[0..i])
            "vfredsum.vs    v22, v14, v22\n\t"  // v22[0] = v22[0] + sum(v14[0..i])
            "vfmv.f.s       ft2, v16\n\t"
            "vfmv.f.s       ft3, v18\n\t"
            "vfmv.f.s       ft4, v20\n\t"
            "vfmv.f.s       ft5, v22\n\t"

            "beqz           %10, 3f\n\t"
            // fuse relu
            "fmax.s         ft2, ft2, ft6\n\t"  // **** relu ****
            "fmax.s         ft3, ft3, ft6\n\t"  // **** relu ****
            "fmax.s         ft4, ft4, ft6\n\t"  // **** relu ****
            "fmax.s         ft5, ft5, ft6\n\t"  // **** relu ****

            "3:\n\t"

            "fsw            ft2, 0(%4)\n\t"
            "fsw            ft3, 4(%4)\n\t"
            "fsw            ft4, 0(%5)\n\t"
            "fsw            ft5, 4(%5)\n\t"

            : "=r"(pa0),          // %0
              "=r"(pa1),          // %1
              "=r"(pb0),          // %2
              "=r"(pb1),          // %3
              "=r"(pc0),          // %4
              "=r"(pc1),          // %5
              "=r"(k8),           // %6
              "=r"(k_tail),       // %7
              "=r"(bias),         // %8
              "=r"(load_stride),  // %9
              "=r"(fuse_relu)     // %10
            : "0"(pa0), "1"(pa1), "2"(pb0), "3"(pb1), "4"(pc0), "5"(pc1), "6"(k8), "7"(k_tail),
              "8"(bias), "9"(load_stride), "10"(fuse_relu)
            : "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8", "v9", "v10", "v11", "v12",
              "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "ft0",
              "ft1", "ft2", "ft3", "ft4", "ft5", "ft6", "t0", "t1", "t6");
        pb += 2 * k;
        pc0 += 2;
        pc1 += 2;
    }
    if (n1 > 0) {
        float *pa0 = sa;
        float *pa1 = pa0 + 1;
        int k8 = k >> 3;
        int k_tail = k & 7;
        int load_stride = 8;
        asm volatile(
            "fmv.w.x        ft4, zero\n\t"  // for fuse relu
            "mv             t5, %5\n\t"     // t5 = k8
            "vsetvli        zero, zero, e32, m2\n\t"
            "vxor.vv        v6, v6, v6\n\t"  // clear
            "vxor.vv        v8, v8, v8\n\t"  // clear
            "flw            ft0, 0(%7)\n\t"  // ft0 = *bias
            "flw            ft1, 4(%7)\n\t"  // ft1 = *(bias + 1)
            "vfmv.s.f       v10, ft0\n\t"    // v10[0] = ft0
            "vfmv.s.f       v12, ft1\n\t"    // v12[0] = ft1

            "beqz           %6, 1f\n\t"  // k_tail == 0 ?
            // Processing k_tail
            "slli           t0, %6, 2\n\t"  // t0 = k_tail * 4
            "slli           t1, t0, 1\n\t"  // t1 = t0 * 2
            "vsetvli        zero, %6, e32, m2\n\t"
            "vlsw.v         v0, (%0), %8\n\t"
            "add            %0, %0, t1\n\t"
            "vlsw.v         v2, (%1), %8\n\t"
            "addi           %1, %0, 4\n\t"

            "vlw.v          v4, (%2)\n\t"
            "add            %2, %2, t0\n\t"

            "vfmacc.vv      v6, v0, v4\n\t"
            "vfmacc.vv      v8, v2, v4\n\t"
            "beqz           t5, 2f\n\t"  // k8 == 0 ?
            "vsetvli        zero, zero, e32, m2\n\t"

            "1:\n\t"
            // start subkernel_m2n1k8
            "vlsw.v         v0, (%0), %8\n\t"
            "addi           %0, %0, 64\n\t"
            "vlsw.v         v2, (%1), %8\n\t"
            "addi           %1, %0, 4\n\t"

            "vlw.v          v4, (%2)\n\t"
            "addi           %2, %2, 32\n\t"

            "vfmacc.vv      v6, v0, v4\n\t"
            "vfmacc.vv      v8, v2, v4\n\t"
            "addi           t5, t5, -1\n\t"
            "bnez           t5, 1b\n\t"

            "2:\n\t"
            // end kernel_m2n1
            "vfredsum.vs    v10, v6, v10\n\t"  // v10[0] = v10[0] + sum(v6[0..i])
            "vfredsum.vs    v12, v8, v12\n\t"  // v12[0] = v12[0] + sum(v8[0..i])
            "vfmv.f.s       ft2, v10\n\t"
            "vfmv.f.s       ft3, v12\n\t"

            "beqz           %9, 3f\n\t"
            // fuse relu
            "fmax.s         ft2, ft3, ft4\n\t"  // **** relu ****
            "fmax.s         ft2, ft3, ft4\n\t"  // **** relu ****

            "3:\n\t"
            "fsw            ft2, 0(%3)\n\t"
            "fsw            ft3, 0(%4)\n\t"

            : "=r"(pa0),          // %0
              "=r"(pa1),          // %1
              "=r"(pb),           // %2
              "=r"(pc0),          // %3
              "=r"(pc1),          // %4
              "=r"(k8),           // %5
              "=r"(k_tail),       // %6
              "=r"(bias),         // %7
              "=r"(load_stride),  // %8
              "=r"(fuse_relu)     // %9
            : "0"(pa0), "1"(pa1), "2"(pb), "3"(pc0), "4"(pc1), "5"(k8), "6"(k_tail), "7"(bias),
              "8"(load_stride), "9"(fuse_relu)
            : "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8", "v9", "v10", "v11", "v12",
              "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "ft0",
              "ft1", "ft2", "ft3", "ft4", "t0", "t1", "t5");
    }
#else
    for (int i = 0; i < n4; i++) {
        pa = sa;
        pc0[0] = pc0[1] = pc0[2] = pc0[3] = *bias;
        pc1[0] = pc1[1] = pc1[2] = pc1[3] = *(bias + 1);
        int j = 0;
        for (; j + 7 < k; j += 8) {
            pc0[0] += pa[0] * pb[0];
            pc1[0] += pa[1] * pb[0];
            pc0[1] += pa[0] * pb[1];
            pc1[1] += pa[1] * pb[1];
            pc0[2] += pa[0] * pb[2];
            pc1[2] += pa[1] * pb[2];
            pc0[3] += pa[0] * pb[3];
            pc1[3] += pa[1] * pb[3];

            pc0[0] += pa[2] * pb[4];
            pc1[0] += pa[3] * pb[4];
            pc0[1] += pa[2] * pb[5];
            pc1[1] += pa[3] * pb[5];
            pc0[2] += pa[2] * pb[6];
            pc1[2] += pa[3] * pb[6];
            pc0[3] += pa[2] * pb[7];
            pc1[3] += pa[3] * pb[7];

            pc0[0] += pa[4] * pb[8];
            pc1[0] += pa[5] * pb[8];
            pc0[1] += pa[4] * pb[9];
            pc1[1] += pa[5] * pb[9];
            pc0[2] += pa[4] * pb[10];
            pc1[2] += pa[5] * pb[10];
            pc0[3] += pa[4] * pb[11];
            pc1[3] += pa[5] * pb[11];

            pc0[0] += pa[6] * pb[12];
            pc1[0] += pa[7] * pb[12];
            pc0[1] += pa[6] * pb[13];
            pc1[1] += pa[7] * pb[13];
            pc0[2] += pa[6] * pb[14];
            pc1[2] += pa[7] * pb[14];
            pc0[3] += pa[6] * pb[15];
            pc1[3] += pa[7] * pb[15];

            pc0[0] += pa[8] * pb[16];
            pc1[0] += pa[9] * pb[16];
            pc0[1] += pa[8] * pb[17];
            pc1[1] += pa[9] * pb[17];
            pc0[2] += pa[8] * pb[18];
            pc1[2] += pa[9] * pb[18];
            pc0[3] += pa[8] * pb[19];
            pc1[3] += pa[9] * pb[19];

            pc0[0] += pa[10] * pb[20];
            pc1[0] += pa[11] * pb[20];
            pc0[1] += pa[10] * pb[21];
            pc1[1] += pa[11] * pb[21];
            pc0[2] += pa[10] * pb[22];
            pc1[2] += pa[11] * pb[22];
            pc0[3] += pa[10] * pb[23];
            pc1[3] += pa[11] * pb[23];

            pc0[0] += pa[12] * pb[24];
            pc1[0] += pa[13] * pb[24];
            pc0[1] += pa[12] * pb[25];
            pc1[1] += pa[13] * pb[25];
            pc0[2] += pa[12] * pb[26];
            pc1[2] += pa[13] * pb[26];
            pc0[3] += pa[12] * pb[27];
            pc1[3] += pa[13] * pb[27];

            pc0[0] += pa[14] * pb[28];
            pc1[0] += pa[15] * pb[28];
            pc0[1] += pa[14] * pb[29];
            pc1[1] += pa[15] * pb[29];
            pc0[2] += pa[14] * pb[30];
            pc1[2] += pa[15] * pb[30];
            pc0[3] += pa[14] * pb[31];
            pc1[3] += pa[15] * pb[31];

            pa += 16;
            pb += 32;
        }
        if (j + 3 < k) {
            j += 4;
            pc0[0] += pa[0] * pb[0];
            pc1[0] += pa[1] * pb[0];
            pc0[1] += pa[0] * pb[1];
            pc1[1] += pa[1] * pb[1];
            pc0[2] += pa[0] * pb[2];
            pc1[2] += pa[1] * pb[2];
            pc0[3] += pa[0] * pb[3];
            pc1[3] += pa[1] * pb[3];

            pc0[0] += pa[2] * pb[4];
            pc1[0] += pa[3] * pb[4];
            pc0[1] += pa[2] * pb[5];
            pc1[1] += pa[3] * pb[5];
            pc0[2] += pa[2] * pb[6];
            pc1[2] += pa[3] * pb[6];
            pc0[3] += pa[2] * pb[7];
            pc1[3] += pa[3] * pb[7];

            pc0[0] += pa[4] * pb[8];
            pc1[0] += pa[5] * pb[8];
            pc0[1] += pa[4] * pb[9];
            pc1[1] += pa[5] * pb[9];
            pc0[2] += pa[4] * pb[10];
            pc1[2] += pa[5] * pb[10];
            pc0[3] += pa[4] * pb[11];
            pc1[3] += pa[5] * pb[11];

            pc0[0] += pa[6] * pb[12];
            pc1[0] += pa[7] * pb[12];
            pc0[1] += pa[6] * pb[13];
            pc1[1] += pa[7] * pb[13];
            pc0[2] += pa[6] * pb[14];
            pc1[2] += pa[7] * pb[14];
            pc0[3] += pa[6] * pb[15];
            pc1[3] += pa[7] * pb[15];

            pa += 8;
            pb += 16;
        }
        if (j + 1 < k) {
            j += 2;
            pc0[0] += pa[0] * pb[0];
            pc1[0] += pa[1] * pb[0];
            pc0[1] += pa[0] * pb[1];
            pc1[1] += pa[1] * pb[1];
            pc0[2] += pa[0] * pb[2];
            pc1[2] += pa[1] * pb[2];
            pc0[3] += pa[0] * pb[3];
            pc1[3] += pa[1] * pb[3];

            pc0[0] += pa[2] * pb[4];
            pc1[0] += pa[3] * pb[4];
            pc0[1] += pa[2] * pb[5];
            pc1[1] += pa[3] * pb[5];
            pc0[2] += pa[2] * pb[6];
            pc1[2] += pa[3] * pb[6];
            pc0[3] += pa[2] * pb[7];
            pc1[3] += pa[3] * pb[7];

            pa += 4;
            pb += 8;
        }
        if (j < k) {
            pc0[0] += pa[0] * pb[0];
            pc1[0] += pa[1] * pb[0];
            pc0[1] += pa[0] * pb[1];
            pc1[1] += pa[1] * pb[1];
            pc0[2] += pa[0] * pb[2];
            pc1[2] += pa[1] * pb[2];
            pc0[3] += pa[0] * pb[3];
            pc1[3] += pa[1] * pb[3];

            pa += 2;
            pb += 4;
        }
        if (fuse_relu) {
            pc0[0] = pc0[0] > 0 ? pc0[0] : 0;
            pc0[1] = pc0[1] > 0 ? pc0[1] : 0;
            pc0[2] = pc0[2] > 0 ? pc0[2] : 0;
            pc0[3] = pc0[3] > 0 ? pc0[3] : 0;

            pc1[0] = pc1[0] > 0 ? pc1[0] : 0;
            pc1[1] = pc1[1] > 0 ? pc1[1] : 0;
            pc1[2] = pc1[2] > 0 ? pc1[2] : 0;
            pc1[3] = pc1[3] > 0 ? pc1[3] : 0;
        }
        pc0 += 4;
        pc1 += 4;
    }
    if (n2 > 0) {
        pa = sa;
        pc0[0] = pc0[1] = *bias;
        pc1[0] = pc1[1] = *(bias + 1);
        float *pb0 = pb;
        float *pb1 = pb0 + k;
        int j = 0;
        for (; j + 7 < k; j += 8) {
            pc0[0] += pa[0] * pb0[0];
            pc1[0] += pa[1] * pb0[0];
            pc0[1] += pa[0] * pb1[0];
            pc1[1] += pa[1] * pb1[0];

            pc0[0] += pa[2] * pb0[1];
            pc1[0] += pa[3] * pb0[1];
            pc0[1] += pa[2] * pb1[1];
            pc1[1] += pa[3] * pb1[1];

            pc0[0] += pa[4] * pb0[2];
            pc1[0] += pa[5] * pb0[2];
            pc0[1] += pa[4] * pb1[2];
            pc1[1] += pa[5] * pb1[2];

            pc0[0] += pa[6] * pb0[3];
            pc1[0] += pa[7] * pb0[3];
            pc0[1] += pa[6] * pb1[3];
            pc1[1] += pa[7] * pb1[3];

            pc0[0] += pa[8] * pb0[4];
            pc1[0] += pa[9] * pb0[4];
            pc0[1] += pa[8] * pb1[4];
            pc1[1] += pa[9] * pb1[4];

            pc0[0] += pa[10] * pb0[5];
            pc1[0] += pa[11] * pb0[5];
            pc0[1] += pa[10] * pb1[5];
            pc1[1] += pa[11] * pb1[5];

            pc0[0] += pa[12] * pb0[6];
            pc1[0] += pa[13] * pb0[6];
            pc0[1] += pa[12] * pb1[6];
            pc1[1] += pa[13] * pb1[6];

            pc0[0] += pa[14] * pb0[7];
            pc1[0] += pa[15] * pb0[7];
            pc0[1] += pa[14] * pb1[7];
            pc1[1] += pa[15] * pb1[7];

            pa += 16;
            pb0 += 8;
            pb1 += 8;
        }
        if (j + 3 < k) {
            j += 4;
            pc0[0] += pa[0] * pb0[0];
            pc1[0] += pa[1] * pb0[0];
            pc0[1] += pa[0] * pb1[0];
            pc1[1] += pa[1] * pb1[0];

            pc0[0] += pa[2] * pb0[1];
            pc1[0] += pa[3] * pb0[1];
            pc0[1] += pa[2] * pb1[1];
            pc1[1] += pa[3] * pb1[1];

            pc0[0] += pa[4] * pb0[2];
            pc1[0] += pa[5] * pb0[2];
            pc0[1] += pa[4] * pb1[2];
            pc1[1] += pa[5] * pb1[2];

            pc0[0] += pa[6] * pb0[3];
            pc1[0] += pa[7] * pb0[3];
            pc0[1] += pa[6] * pb1[3];
            pc1[1] += pa[7] * pb1[3];

            pa += 8;
            pb0 += 4;
            pb1 += 4;
        }
        if (j + 1 < k) {
            j += 2;
            pc0[0] += pa[0] * pb0[0];
            pc1[0] += pa[1] * pb0[0];
            pc0[1] += pa[0] * pb1[0];
            pc1[1] += pa[1] * pb1[0];

            pc0[0] += pa[2] * pb0[1];
            pc1[0] += pa[3] * pb0[1];
            pc0[1] += pa[2] * pb1[1];
            pc1[1] += pa[3] * pb1[1];

            pa += 4;
            pb0 += 2;
            pb1 += 2;
        }
        if (j < k) {
            pc0[0] += pa[0] * pb0[0];
            pc1[0] += pa[1] * pb0[0];
            pc0[1] += pa[0] * pb1[0];
            pc1[1] += pa[1] * pb1[0];

            pa += 2;
            pb0 += 1;
            pb1 += 1;
        }
        if (fuse_relu) {
            pc0[0] = pc0[0] > 0 ? pc0[0] : 0;
            pc0[1] = pc0[1] > 0 ? pc0[1] : 0;
            pc1[0] = pc1[0] > 0 ? pc1[0] : 0;
            pc1[1] = pc1[1] > 0 ? pc1[1] : 0;
        }
        pc0 += 2;
        pc1 += 2;
        pb += 2 * k;
    }
    if (n1 > 0) {
        pa = sa;
        pc0[0] = *bias;
        pc1[0] = *(bias + 1);
        int j = 0;
        for (; j + 7 < k; j += 8) {
            pc0[0] += pa[0] * pb[0];
            pc1[0] += pa[1] * pb[0];

            pc0[0] += pa[2] * pb[1];
            pc1[0] += pa[3] * pb[1];

            pc0[0] += pa[4] * pb[2];
            pc1[0] += pa[5] * pb[2];

            pc0[0] += pa[6] * pb[3];
            pc1[0] += pa[7] * pb[3];

            pc0[0] += pa[8] * pb[4];
            pc1[0] += pa[9] * pb[4];

            pc0[0] += pa[10] * pb[5];
            pc1[0] += pa[11] * pb[5];

            pc0[0] += pa[12] * pb[6];
            pc1[0] += pa[13] * pb[6];

            pc0[0] += pa[14] * pb[7];
            pc1[0] += pa[15] * pb[7];

            pa += 16;
            pb += 8;
        }
        if (j + 3 < k) {
            j += 4;
            pc0[0] += pa[0] * pb[0];
            pc1[0] += pa[1] * pb[0];

            pc0[0] += pa[2] * pb[1];
            pc1[0] += pa[3] * pb[1];

            pc0[0] += pa[4] * pb[2];
            pc1[0] += pa[5] * pb[2];

            pc0[0] += pa[6] * pb[3];
            pc1[0] += pa[7] * pb[3];

            pa += 8;
            pb += 4;
        }
        if (j + 1 < k) {
            j += 2;
            pc0[0] += pa[0] * pb[0];
            pc1[0] += pa[1] * pb[0];

            pc0[0] += pa[2] * pb[1];
            pc1[0] += pa[3] * pb[1];

            pa += 4;
            pb += 2;
        }
        if (j < k) {
            pc0[0] += pa[0] * pb[0];
            pc1[0] += pa[1] * pb[0];

            pa += 2;
            pb += 1;
        }
        if (fuse_relu) {
            pc0[0] = pc0[0] > 0 ? pc0[0] : 0;
            pc1[0] = pc1[0] > 0 ? pc1[0] : 0;
        }
        pc0 += 1;
        pc1 += 1;
    }
#endif  // __riscv_vector
}

static inline void kernel_m4_f32(float *dst, float *sa, float *sb, int m, int k, int n, int ldc,
                                 float *bias, bool fuse_relu)
{
    float *pa = sa;
    float *pb = sb;
    float *pc0 = dst;
    float *pc1 = pc0 + ldc;
    float *pc2 = pc1 + ldc;
    float *pc3 = pc2 + ldc;
    DECOMPOSE_K
    DECOMPOSE_N

#if __riscv_vector == 128
    if (n4 > 0) {
        asm volatile(
            "vsetvli        zero, zero, e32, m1\n\t"
            "flw            ft8, (%11)\n\t"
            "flw            ft9, 4(%11)\n\t"
            "flw            ft10, 8(%11)\n\t"
            "flw            ft11, 12(%11)\n\t"
            "beqz           %12, 1f\n\t"   // if fuse_relu == 0
            "vmv.v.x        v0, zero\n\t"  // v0 hold const zero, using for relu

            "1:\n\t"                        // n4
                                            // start kernel_m4n4
            "vfmv.v.f       v24, ft8\n\t"   // v24[0..3] = *bias
            "vfmv.v.f       v25, ft9\n\t"   // v25[0..3] = *(bias + 1)
            "vfmv.v.f       v26, ft10\n\t"  // v26[0..3] = *(bias + 2)
            "vfmv.v.f       v27, ft11\n\t"  // v27[0..3] = *(bias + 3)
            // "vlw.v          v24, (%11)\n\t"     // v24[0..3] = bias[0..3]
            // "vlw.v          v25, (%11)\n\t"     // v25[0..3] = bias[0..3]
            // "vlw.v          v26, (%11)\n\t"     // v26[0..3] = bias[0..3]
            // "vlw.v          v27, (%11)\n\t"     // v27[0..3] = bias[0..3]
            // "addi           %11, %11, 16\n\t"   // bias += 4 * 4

            "mv             a1, %0\n\t"  // a1 = pa
            "mv             t0, %6\n\t"  // t0 = k8

            "flw            ft0, (a1)\n\t"
            "flw            ft1, 4(a1)\n\t"
            "flw            ft2, 8(a1)\n\t"
            "flw            ft3, 12(a1)\n\t"  // pre load pa

            "beqz           t0, 3f\n\t"  // k8 == 0 ?

            "vlw.v          v1, (%1)\n\t"  // pre load pb
            "addi           %1, %1, 16\n\t"

            "2:\n\t"
            // start subkernel_m4n4k8

            "vlw.v          v2, (%1)\n\t"  // load pb
            "addi           %1, %1, 16\n\t"
            "flw            ft4, 16(a1)\n\t"
            "vfmacc.vf      v24, ft0, v1\n\t"
            "flw            ft5, 20(a1)\n\t"
            "vfmacc.vf      v25, ft1, v1\n\t"
            "flw            ft6, 24(a1)\n\t"
            "vfmacc.vf      v26, ft2, v1\n\t"
            "flw            ft7, 28(a1)\n\t"
            "vfmacc.vf      v27, ft3, v1\n\t"  // 0

            "vlw.v          v3, (%1)\n\t"
            "addi           %1, %1, 16\n\t"
            "flw            ft0, 32(a1)\n\t"
            "vfmacc.vf      v24, ft4, v2\n\t"
            "flw            ft1, 36(a1)\n\t"
            "vfmacc.vf      v25, ft5, v2\n\t"
            "flw            ft2, 40(a1)\n\t"
            "vfmacc.vf      v26, ft6, v2\n\t"
            "flw            ft3, 44(a1)\n\t"
            "vfmacc.vf      v27, ft7, v2\n\t"  // 1

            "vlw.v          v4, (%1)\n\t"
            "addi           %1, %1, 16\n\t"
            "flw            ft4, 48(a1)\n\t"
            "vfmacc.vf      v24, ft0, v3\n\t"
            "flw            ft5, 52(a1)\n\t"
            "vfmacc.vf      v25, ft1, v3\n\t"
            "flw            ft6, 56(a1)\n\t"
            "vfmacc.vf      v26, ft2, v3\n\t"
            "flw            ft7, 60(a1)\n\t"
            "vfmacc.vf      v27, ft3, v3\n\t"  // 2

            "vlw.v          v5, (%1)\n\t"
            "addi           %1, %1, 16\n\t"
            "flw            ft0, 64(a1)\n\t"
            "vfmacc.vf      v24, ft4, v4\n\t"
            "flw            ft1, 68(a1)\n\t"
            "vfmacc.vf      v25, ft5, v4\n\t"
            "flw            ft2, 72(a1)\n\t"
            "vfmacc.vf      v26, ft6, v4\n\t"
            "flw            ft3, 76(a1)\n\t"
            "vfmacc.vf      v27, ft7, v4\n\t"  // 3

            "vlw.v          v6, (%1)\n\t"
            "addi           %1, %1, 16\n\t"
            "flw            ft4, 80(a1)\n\t"
            "vfmacc.vf      v24, ft0, v5\n\t"
            "flw            ft5, 84(a1)\n\t"
            "vfmacc.vf      v25, ft1, v5\n\t"
            "flw            ft6, 88(a1)\n\t"
            "vfmacc.vf      v26, ft2, v5\n\t"
            "flw            ft7, 92(a1)\n\t"
            "vfmacc.vf      v27, ft3, v5\n\t"  // 4

            "vlw.v          v7, (%1)\n\t"
            "addi           %1, %1, 16\n\t"
            "flw            ft0, 96(a1)\n\t"
            "vfmacc.vf      v24, ft4, v6\n\t"
            "flw            ft1, 100(a1)\n\t"
            "vfmacc.vf      v25, ft5, v6\n\t"
            "flw            ft2, 104(a1)\n\t"
            "vfmacc.vf      v26, ft6, v6\n\t"
            "flw            ft3, 108(a1)\n\t"
            "vfmacc.vf      v27, ft7, v6\n\t"  // 5

            "vlw.v          v8, (%1)\n\t"
            "addi           %1, %1, 16\n\t"
            "flw            ft4, 112(a1)\n\t"
            "vfmacc.vf      v24, ft0, v7\n\t"
            "flw            ft5, 116(a1)\n\t"
            "vfmacc.vf      v25, ft1, v7\n\t"
            "flw            ft6, 120(a1)\n\t"
            "vfmacc.vf      v26, ft2, v7\n\t"
            "flw            ft7, 124(a1)\n\t"
            "vfmacc.vf      v27, ft3, v7\n\t"  // 6
            "addi           a1, a1, 128\n\t"   // += 32 elements, bump pa to next k8 addr

            "vlw.v          v1, (%1)\n\t"
            "addi           %1, %1, 16\n\t"
            "flw            ft0, (a1)\n\t"
            "vfmacc.vf      v24, ft4, v8\n\t"
            "flw            ft1, 4(a1)\n\t"
            "vfmacc.vf      v25, ft5, v8\n\t"
            "flw            ft2, 8(a1)\n\t"
            "vfmacc.vf      v26, ft6, v8\n\t"
            "flw            ft3, 12(a1)\n\t"
            "vfmacc.vf      v27, ft7, v8\n\t"  // 7

            "addi           t0, t0, -1\n\t"  // k8 --
            "bnez           t0, 2b\n\t"

            "addi           %1, %1, -16\n\t"  // pb -= 4  ********* bump pb to origin addr
                                              // ************

            "3:\n\t"
            "beqz           %7, 4f\n\t"  // k4 == 0 ?
            // start subkernel_m4n4k4
            "vlw.v          v1, (%1)\n\t"
            "addi           %1, %1, 16\n\t"
            "flw            ft4, 16(a1)\n\t"
            "vfmacc.vf      v24, ft0, v1\n\t"
            "flw            ft5, 20(a1)\n\t"
            "vfmacc.vf      v25, ft1, v1\n\t"
            "flw            ft6, 24(a1)\n\t"
            "vfmacc.vf      v26, ft2, v1\n\t"
            "flw            ft7, 28(a1)\n\t"
            "vfmacc.vf      v27, ft3, v1\n\t"  // 0

            "vlw.v          v2, (%1)\n\t"
            "addi           %1, %1, 16\n\t"
            "flw            ft0, 32(a1)\n\t"
            "vfmacc.vf      v24, ft4, v2\n\t"
            "flw            ft1, 36(a1)\n\t"
            "vfmacc.vf      v25, ft5, v2\n\t"
            "flw            ft2, 40(a1)\n\t"
            "vfmacc.vf      v26, ft6, v2\n\t"
            "flw            ft3, 44(a1)\n\t"
            "vfmacc.vf      v27, ft7, v2\n\t"  // 1

            "vlw.v          v3, (%1)\n\t"
            "addi           %1, %1, 16\n\t"
            "flw            ft4, 48(a1)\n\t"
            "vfmacc.vf      v24, ft0, v3\n\t"
            "flw            ft5, 52(a1)\n\t"
            "vfmacc.vf      v25, ft1, v3\n\t"
            "flw            ft6, 56(a1)\n\t"
            "vfmacc.vf      v26, ft2, v3\n\t"
            "flw            ft7, 60(a1)\n\t"
            "vfmacc.vf      v27, ft3, v3\n\t"  // 2
            "addi           a1, a1, 64\n\t"    // += 16 elements, bump pa to next k addr

            "vlw.v          v4, (%1)\n\t"
            "addi           %1, %1, 16\n\t"
            "flw            ft0, (a1)\n\t"
            "vfmacc.vf      v24, ft4, v4\n\t"
            "flw            ft1, 4(a1)\n\t"
            "vfmacc.vf      v25, ft5, v4\n\t"
            "flw            ft2, 8(a1)\n\t"
            "vfmacc.vf      v26, ft6, v4\n\t"
            "flw            ft3, 12(a1)\n\t"
            "vfmacc.vf      v27, ft7, v4\n\t"  // 3

            "4:\n\t"
            "beqz           %8, 5f\n\t"  // k2 == 0 ?
            // start subkernel_m4n4k2

            "vlw.v          v1, (%1)\n\t"
            "addi           %1, %1, 16\n\t"

            "flw            ft4, 16(a1)\n\t"
            "vfmacc.vf      v24, ft0, v1\n\t"
            "flw            ft5, 20(a1)\n\t"
            "vfmacc.vf      v25, ft1, v1\n\t"
            "flw            ft6, 24(a1)\n\t"
            "vfmacc.vf      v26, ft2, v1\n\t"
            "flw            ft7, 28(a1)\n\t"
            "vfmacc.vf      v27, ft3, v1\n\t"  // 0
            "addi           a1, a1, 32\n\t"    // += 8 elements, bump pa to next k addr

            "vlw.v          v2, (%1)\n\t"
            "addi           %1, %1, 16\n\t"
            "flw            ft0, (a1)\n\t"
            "vfmacc.vf      v24, ft4, v2\n\t"
            "flw            ft1, 4(a1)\n\t"
            "vfmacc.vf      v25, ft5, v2\n\t"
            "flw            ft2, 8(a1)\n\t"
            "vfmacc.vf      v26, ft6, v2\n\t"
            "flw            ft3, 12(a1)\n\t"
            "vfmacc.vf      v27, ft7, v2\n\t"  // 1

            "5:\n\t"
            "beqz           %9, 6f\n\t"  // k1 == 0 ?
            // start subkernel_m4n4k1
            "vlw.v          v1, (%1)\n\t"
            "addi           %1, %1, 16\n\t"

            "vfmacc.vf      v24, ft0, v1\n\t"
            "vfmacc.vf      v25, ft1, v1\n\t"
            "vfmacc.vf      v26, ft2, v1\n\t"
            "vfmacc.vf      v27, ft3, v1\n\t"  // 0

            "6:\n\t"
            "beqz           %12, 7f\n\t"
            // fused relu
            "vfmax.vv       v24, v24, v0\n\t"  // **** relu ****
            "vfmax.vv       v25, v25, v0\n\t"  // **** relu ****
            "vfmax.vv       v26, v26, v0\n\t"  // **** relu ****
            "vfmax.vv       v27, v27, v0\n\t"  // **** relu ****

            "7:\n\t"
            // end kernel_m4n4
            "vsw.v          v24, (%2)\n\t"
            "addi           %2, %2, 16\n\t"
            "vsw.v          v25, (%3)\n\t"
            "addi           %3, %3, 16\n\t"
            "vsw.v          v26, (%4)\n\t"
            "addi           %4, %4, 16\n\t"
            "vsw.v          v27, (%5)\n\t"
            "addi           %5, %5, 16\n\t"

            "addi           %10, %10, -1\n\t"
            "bnez           %10, 1b\n\t"

            : "=r"(pa),        // %0
              "=r"(pb),        // %1
              "=r"(pc0),       // %2
              "=r"(pc1),       // %3
              "=r"(pc2),       // %4
              "=r"(pc3),       // %5
              "=r"(k8),        // %6
              "=r"(k4),        // %7
              "=r"(k2),        // %8
              "=r"(k1),        // %9
              "=r"(n4),        // %10
              "=r"(bias),      // %11
              "=r"(fuse_relu)  // %12
            : "0"(pa), "1"(pb), "2"(pc0), "3"(pc1), "4"(pc2), "5"(pc3), "6"(k8), "7"(k4), "8"(k2),
              "9"(k1), "10"(n4), "11"(bias), "12"(fuse_relu)
            : "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8", "v24", "v25", "v26", "v27",
              "a1", "t0", "ft0", "ft1", "ft2", "ft3", "ft4", "ft5", "ft6", "ft7", "ft8", "ft9",
              "ft10", "ft11");
    }
    if (n2 > 0) {
        float *pa = sa;
        float *pb0 = pb;
        float *pb1 = pb0 + k;
        float *pc00 = pc0;
        float *pc11 = pc00 + 1;
        asm volatile(
            "slli           t1, %10, 2\n\t"
            "vsetvli        zero, zero, e32, m1\n\t"
            // "flw            ft8, (%9)\n\t"
            // "flw            ft9, 4(%9)\n\t"
            // "addi           %9, %9, 8\n\t"

            "vlw.v          v24, (%9)\n\t"  // v24[0..3] = bias[0]..bias[3]
            "vlw.v          v25, (%9)\n\t"  // v25[0..3] = bias[0]..bias[3]
            // "vfmv.v.f       v24, ft8\n\t"       // v24[0..3] = bias[0];
            // "vfmv.v.f       v25, ft9\n\t"       // v25[0..3] = bias[1];

            "flw            ft0, (%1)\n\t"  // pre load pb0
            "flw            fa0, (%2)\n\t"  // pre load pb1

            "beqz           %11, 0f\n\t"   // if fuse_relu == 0
            "vmv.v.x        v0, zero\n\t"  // v0 hold const zero, using for relu

            "0:\n\t"
            "mv             t0, %5\n\t"  // t0 = k8
            "beqz           t0, 2f\n\t"  // k8 == 0 ?

            "1:\n\t"
            // start subkernel_m4n2k8
            "vlw.v          v1, (%0)\n\t"  // load pa
            "addi           %0, %0, 16\n\t"
            "flw            ft1, 4(%1)\n\t"
            "vfmacc.vf      v24, ft0, v1\n\t"
            "flw            fa1, 4(%2)\n\t"
            "vfmacc.vf      v25, fa0, v1\n\t"  // 0

            "vlw.v          v2, (%0)\n\t"
            "addi           %0, %0, 16\n\t"
            "flw            ft0, 8(%1)\n\t"
            "vfmacc.vf      v24, ft1, v2\n\t"
            "flw            fa0, 8(%2)\n\t"
            "vfmacc.vf      v25, fa1, v2\n\t"  // 1

            "vlw.v          v3, (%0)\n\t"
            "addi           %0, %0, 16\n\t"
            "flw            ft1, 12(%1)\n\t"
            "vfmacc.vf      v24, ft0, v3\n\t"
            "flw            fa1, 12(%2)\n\t"
            "vfmacc.vf      v25, fa0, v3\n\t"  // 2

            "vlw.v          v4, (%0)\n\t"
            "addi           %0, %0, 16\n\t"
            "flw            ft0, 16(%1)\n\t"
            "vfmacc.vf      v24, ft1, v4\n\t"
            "flw            fa0, 16(%2)\n\t"
            "vfmacc.vf      v25, fa1, v4\n\t"  // 3

            "vlw.v          v5, (%0)\n\t"
            "addi           %0, %0, 16\n\t"
            "flw            ft1, 20(%1)\n\t"
            "vfmacc.vf      v24, ft0, v5\n\t"
            "flw            fa1, 20(%2)\n\t"
            "vfmacc.vf      v25, fa0, v5\n\t"  // 4

            "vlw.v          v6, (%0)\n\t"
            "addi           %0, %0, 16\n\t"
            "flw            ft0, 24(%1)\n\t"
            "vfmacc.vf      v24, ft1, v6\n\t"
            "flw            fa0, 24(%2)\n\t"
            "vfmacc.vf      v25, fa1, v6\n\t"  // 5

            "vlw.v          v7, (%0)\n\t"
            "addi           %0, %0, 16\n\t"
            "flw            ft1, 28(%1)\n\t"
            "vfmacc.vf      v24, ft0, v7\n\t"
            "flw            fa1, 28(%2)\n\t"
            "vfmacc.vf      v25, fa0, v7\n\t"  // 6
            "addi           %1, %1, 32\n\t"    // += 8 elements, bump pb0 to next k8 addr
            "addi           %2, %2, 32\n\t"    // += 8 elements, bump pb1 to next k8 addr

            "vlw.v          v8, (%0)\n\t"
            "addi           %0, %0, 16\n\t"
            "flw            ft0, (%1)\n\t"
            "vfmacc.vf      v24, ft1, v8\n\t"
            "flw            fa0, (%2)\n\t"
            "vfmacc.vf      v25, fa1, v8\n\t"  // 7

            "addi           t0, t0, -1\n\t"
            "bnez           t0, 1b\n\t"

            "2:\n\t"
            "beqz           %6, 3f\n\t"  // k4 == 0 ?
            // start subkernel_m4n2k4
            "vlw.v          v1, (%0)\n\t"
            "addi           %0, %0, 16\n\t"
            "flw            ft1, 4(%1)\n\t"
            "vfmacc.vf      v24, ft0, v1\n\t"
            "flw            fa1, 4(%2)\n\t"
            "vfmacc.vf      v25, fa0, v1\n\t"  // 0

            "vlw.v          v2, (%0)\n\t"
            "addi           %0, %0, 16\n\t"
            "flw            ft0, 8(%1)\n\t"
            "vfmacc.vf      v24, ft1, v2\n\t"
            "flw            fa0, 8(%2)\n\t"
            "vfmacc.vf      v25, fa1, v2\n\t"  // 1

            "vlw.v          v3, (%0)\n\t"
            "addi           %0, %0, 16\n\t"
            "flw            ft1, 12(%1)\n\t"
            "vfmacc.vf      v24, ft0, v3\n\t"
            "flw            fa1, 12(%2)\n\t"
            "vfmacc.vf      v25, fa0, v3\n\t"  // 2
            "addi           %1, %1, 16\n\t"    // += 4 elements, bump pb0 to next k addr
            "addi           %2, %2, 16\n\t"    // += 4 elements, bump pb1 to next k addr

            "vlw.v          v4, (%0)\n\t"
            "addi           %0, %0, 16\n\t"
            "flw            ft0, (%1)\n\t"
            "vfmacc.vf      v24, ft1, v4\n\t"
            "flw            fa0, (%2)\n\t"
            "vfmacc.vf      v25, fa1, v4\n\t"  // 3

            "3:\n\t"
            "beqz           %7, 4f\n\t"  // k2 == 0 ?
            // start subkernel_m4n2k2
            "vlw.v          v1, (%0)\n\t"
            "addi           %0, %0, 16\n\t"
            "flw            ft1, 4(%1)\n\t"
            "vfmacc.vf      v24, ft0, v1\n\t"
            "flw            fa1, 4(%2)\n\t"
            "vfmacc.vf      v25, fa0, v1\n\t"  // 0
            "addi           %1, %1, 8\n\t"     // += 2 elements, bump pb0 to next k addr
            "addi           %2, %2, 8\n\t"     // += 2 elements, bump pb1 to next k addr

            "vlw.v          v2, (%0)\n\t"
            "addi           %0, %0, 16\n\t"
            "flw            ft0, (%1)\n\t"
            "vfmacc.vf      v24, ft1, v2\n\t"
            "flw            fa0, (%2)\n\t"
            "vfmacc.vf      v25, fa1, v2\n\t"  // 1

            "4:\n\t"
            "beqz           %8, 5f\n\t"  // k1 == 0 ?
            // start subkernel_m4n2k1
            "vlw.v          v1, (%0)\n\t"
            "addi           %0, %0, 16\n\t"

            "vfmacc.vf      v24, ft0, v1\n\t"
            "vfmacc.vf      v25, fa0, v1\n\t"  // 0

            "5:\n\t"
            "beqz           %11, 6f\n\t"
            // fused relu
            "vfmax.vv       v24, v24, v0\n\t"  // **** relu ****
            "vfmax.vv       v25, v25, v0\n\t"  // **** relu ****

            "6:\n\t"
            "vssw.v v24, (%3), t1\n\t"
            "vssw.v v25, (%4), t1\n\t"

            : "=r"(pa),        // %0
              "=r"(pb0),       // %1
              "=r"(pb1),       // %2
              "=r"(pc00),      // %3
              "=r"(pc11),      // %4
              "=r"(k8),        // %5
              "=r"(k4),        // %6
              "=r"(k2),        // %7
              "=r"(k1),        // %8
              "=r"(bias),      // %9
              "=r"(ldc),       // %10
              "=r"(fuse_relu)  // %11
            : "0"(pa), "1"(pb0), "2"(pb1), "3"(pc00), "4"(pc11), "5"(k8), "6"(k4), "7"(k2), "8"(k1),
              "9"(bias), "10"(ldc), "11"(fuse_relu)
            : "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8", "v24", "v25", "t0", "t1", "ft0",
              "ft1", "fa0", "fa1");
        pb += 2 * k;
        pc0 += 2;
        pc1 += 2;
        pc2 += 2;
        pc3 += 2;
    }
    if (n1 > 0) {
        pa = sa;
        float *pc00 = pc0;
        asm volatile(
            "slli           t1, %8, 2\n\t"  // t1 = ldc * 4
            "vsetvli        zero, zero, e32, m1\n\t"
            // "flw            ft8, 0(%7)\n\t"
            // "vfmv.v.f       v16, ft8\n\t"
            "vlw.v          v16, (%7)\n\t"  // v24[0..3] = bias[0]..bias[3]
            "flw            ft0, (%1)\n\t"  // pre load pb

            "beqz           %9, 0f\n\t"    // if fuse_relu == 0
            "vmv.v.x        v0, zero\n\t"  // v0 hold const zero, using for relu

            "0:\n\t"
            "beqz           %3, 2f\n\t"  // k8 == 0 ?

            "1:\n\t"
            // start subkernel_m4n1k8
            "vlw.v          v1, (%0)\n\t"  // load pa
            "addi           %0, %0, 16\n\t"
            "flw            ft1, 4(%1)\n\t"
            "vfmacc.vf      v16, ft0, v1\n\t"  // 0

            "vlw.v          v2, (%0)\n\t"
            "addi           %0, %0, 16\n\t"
            "flw            ft0, 8(%1)\n\t"
            "vfmacc.vf      v16, ft1, v2\n\t"  // 1

            "vlw.v          v3, (%0)\n\t"
            "addi           %0, %0, 16\n\t"
            "flw            ft1, 12(%1)\n\t"
            "vfmacc.vf      v16, ft0, v3\n\t"  // 2

            "vlw.v          v4, (%0)\n\t"
            "addi           %0, %0, 16\n\t"
            "flw            ft0, 16(%1)\n\t"
            "vfmacc.vf      v16, ft1, v4\n\t"  // 3

            "vlw.v          v5, (%0)\n\t"
            "addi           %0, %0, 16\n\t"
            "flw            ft1, 20(%1)\n\t"
            "vfmacc.vf      v16, ft0, v5\n\t"  // 4

            "vlw.v          v6, (%0)\n\t"
            "addi           %0, %0, 16\n\t"
            "flw            ft0, 24(%1)\n\t"
            "vfmacc.vf      v16, ft1, v6\n\t"  // 5

            "vlw.v          v7, (%0)\n\t"
            "addi           %0, %0, 16\n\t"
            "flw            ft1, 28(%1)\n\t"
            "vfmacc.vf      v16, ft0, v7\n\t"  // 6
            "addi           %1, %1, 32\n\t"    // += 8 elements, bump pb to next k8 addr

            "vlw.v          v8, (%0)\n\t"
            "addi           %0, %0, 16\n\t"
            "flw            ft0, (%1)\n\t"
            "vfmacc.vf      v16, ft1, v8\n\t"  // 7

            "addi           %3, %3, -1\n\t"
            "bnez           %3, 1b\n\t"

            "2:\n\t"
            "beqz           %4, 3f\n\t"  // k4 == 0 ?
            // start subkernel_m4n1k4
            "vlw.v          v1, (%0)\n\t"
            "addi           %0, %0, 16\n\t"
            "flw            ft1, 4(%1)\n\t"
            "vfmacc.vf      v16, ft0, v1\n\t"  // 0

            "vlw.v          v2, (%0)\n\t"
            "addi           %0, %0, 16\n\t"
            "flw            ft0, 8(%1)\n\t"
            "vfmacc.vf      v16, ft1, v2\n\t"  // 1

            "vlw.v          v3, (%0)\n\t"
            "addi           %0, %0, 16\n\t"
            "flw            ft1, 12(%1)\n\t"
            "vfmacc.vf      v16, ft0, v3\n\t"  // 2
            "addi           %1, %1, 16\n\t"    // += 4 elements, bump pb to next k addr

            "vlw.v          v4, (%0)\n\t"
            "addi           %0, %0, 16\n\t"
            "flw            ft0, (%1)\n\t"
            "vfmacc.vf      v16, ft1, v4\n\t"  // 3

            "3:\n\t"
            "beqz           %5, 4f\n\t"  // k2 == 0 ?
            // start subkernel_m4n1k2
            "vlw.v          v1, (%0)\n\t"
            "addi           %0, %0, 16\n\t"
            "flw            ft1, 4(%1)\n\t"
            "vfmacc.vf      v16, ft0, v1\n\t"  // 0
            "addi           %1, %1, 8\n\t"     // += 2 elements, bump pb to next k addr

            "vlw.v          v2, (%0)\n\t"
            "addi           %0, %0, 16\n\t"
            "flw            ft0, (%1)\n\t"
            "vfmacc.vf      v16, ft1, v2\n\t"  // 1

            "4:\n\t"
            "beqz           %6, 5f\n\t"  // k1 == 0 ?
            // start subkernel_m4n2k1
            "vlw.v          v1, (%0)\n\t"
            "addi           %0, %0, 16\n\t"

            "vfmacc.vf      v16, ft0, v1\n\t"  // 0

            "5:\n\t"
            "beqz           %9, 6f\n\t"
            // fused relu
            "vfmax.vv       v16, v16, v0\n\t"  // **** relu ****

            "6:\n\t"
            "vssw.v v16, (%2), t1\n\t"

            : "=r"(pa),        // %0
              "=r"(pb),        // %1
              "=r"(pc00),      // %2
              "=r"(k8),        // %3
              "=r"(k4),        // %4
              "=r"(k2),        // %5
              "=r"(k1),        // %6
              "=r"(bias),      // %7
              "=r"(ldc),       // %8
              "=r"(fuse_relu)  // %9
            : "0"(pa), "1"(pb), "2"(pc00), "3"(k8), "4"(k4), "5"(k2), "6"(k1), "7"(bias), "8"(ldc),
              "9"(fuse_relu)
            : "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8", "v16", "t0", "t1", "ft0",
              "ft1");
    }
#else
    for (int i = 0; i < n4; i++) {
        pa = sa;
        pc0[0] = pc0[1] = pc0[2] = pc0[3] = *bias;
        pc1[0] = pc1[1] = pc1[2] = pc1[3] = *(bias + 1);
        pc2[0] = pc2[1] = pc2[2] = pc2[3] = *(bias + 2);
        pc3[0] = pc3[1] = pc3[2] = pc3[3] = *(bias + 3);
        int j = 0;
        for (; j + 7 < k; j += 8) {
            pc0[0] += pa[0] * pb[0];
            pc1[0] += pa[1] * pb[0];
            pc2[0] += pa[2] * pb[0];
            pc3[0] += pa[3] * pb[0];
            pc0[1] += pa[0] * pb[1];
            pc1[1] += pa[1] * pb[1];
            pc2[1] += pa[2] * pb[1];
            pc3[1] += pa[3] * pb[1];
            pc0[2] += pa[0] * pb[2];
            pc1[2] += pa[1] * pb[2];
            pc2[2] += pa[2] * pb[2];
            pc3[2] += pa[3] * pb[2];
            pc0[3] += pa[0] * pb[3];
            pc1[3] += pa[1] * pb[3];
            pc2[3] += pa[2] * pb[3];
            pc3[3] += pa[3] * pb[3];

            pc0[0] += pa[4] * pb[4];
            pc1[0] += pa[5] * pb[4];
            pc2[0] += pa[6] * pb[4];
            pc3[0] += pa[7] * pb[4];
            pc0[1] += pa[4] * pb[5];
            pc1[1] += pa[5] * pb[5];
            pc2[1] += pa[6] * pb[5];
            pc3[1] += pa[7] * pb[5];
            pc0[2] += pa[4] * pb[6];
            pc1[2] += pa[5] * pb[6];
            pc2[2] += pa[6] * pb[6];
            pc3[2] += pa[7] * pb[6];
            pc0[3] += pa[4] * pb[7];
            pc1[3] += pa[5] * pb[7];
            pc2[3] += pa[6] * pb[7];
            pc3[3] += pa[7] * pb[7];

            pc0[0] += pa[8] * pb[8];
            pc1[0] += pa[9] * pb[8];
            pc2[0] += pa[10] * pb[8];
            pc3[0] += pa[11] * pb[8];
            pc0[1] += pa[8] * pb[9];
            pc1[1] += pa[9] * pb[9];
            pc2[1] += pa[10] * pb[9];
            pc3[1] += pa[11] * pb[9];
            pc0[2] += pa[8] * pb[10];
            pc1[2] += pa[9] * pb[10];
            pc2[2] += pa[10] * pb[10];
            pc3[2] += pa[11] * pb[10];
            pc0[3] += pa[8] * pb[11];
            pc1[3] += pa[9] * pb[11];
            pc2[3] += pa[10] * pb[11];
            pc3[3] += pa[11] * pb[11];

            pc0[0] += pa[12] * pb[12];
            pc1[0] += pa[13] * pb[12];
            pc2[0] += pa[14] * pb[12];
            pc3[0] += pa[15] * pb[12];
            pc0[1] += pa[12] * pb[13];
            pc1[1] += pa[13] * pb[13];
            pc2[1] += pa[14] * pb[13];
            pc3[1] += pa[15] * pb[13];
            pc0[2] += pa[12] * pb[14];
            pc1[2] += pa[13] * pb[14];
            pc2[2] += pa[14] * pb[14];
            pc3[2] += pa[15] * pb[14];
            pc0[3] += pa[12] * pb[15];
            pc1[3] += pa[13] * pb[15];
            pc2[3] += pa[14] * pb[15];
            pc3[3] += pa[15] * pb[15];

            pc0[0] += pa[16] * pb[16];
            pc1[0] += pa[17] * pb[16];
            pc2[0] += pa[18] * pb[16];
            pc3[0] += pa[19] * pb[16];
            pc0[1] += pa[16] * pb[17];
            pc1[1] += pa[17] * pb[17];
            pc2[1] += pa[18] * pb[17];
            pc3[1] += pa[19] * pb[17];
            pc0[2] += pa[16] * pb[18];
            pc1[2] += pa[17] * pb[18];
            pc2[2] += pa[18] * pb[18];
            pc3[2] += pa[19] * pb[18];
            pc0[3] += pa[16] * pb[19];
            pc1[3] += pa[17] * pb[19];
            pc2[3] += pa[18] * pb[19];
            pc3[3] += pa[19] * pb[19];

            pc0[0] += pa[20] * pb[20];
            pc1[0] += pa[21] * pb[20];
            pc2[0] += pa[22] * pb[20];
            pc3[0] += pa[23] * pb[20];
            pc0[1] += pa[20] * pb[21];
            pc1[1] += pa[21] * pb[21];
            pc2[1] += pa[22] * pb[21];
            pc3[1] += pa[23] * pb[21];
            pc0[2] += pa[20] * pb[22];
            pc1[2] += pa[21] * pb[22];
            pc2[2] += pa[22] * pb[22];
            pc3[2] += pa[23] * pb[22];
            pc0[3] += pa[20] * pb[23];
            pc1[3] += pa[21] * pb[23];
            pc2[3] += pa[22] * pb[23];
            pc3[3] += pa[23] * pb[23];

            pc0[0] += pa[24] * pb[24];
            pc1[0] += pa[25] * pb[24];
            pc2[0] += pa[26] * pb[24];
            pc3[0] += pa[27] * pb[24];
            pc0[1] += pa[24] * pb[25];
            pc1[1] += pa[25] * pb[25];
            pc2[1] += pa[26] * pb[25];
            pc3[1] += pa[27] * pb[25];
            pc0[2] += pa[24] * pb[26];
            pc1[2] += pa[25] * pb[26];
            pc2[2] += pa[26] * pb[26];
            pc3[2] += pa[27] * pb[26];
            pc0[3] += pa[24] * pb[27];
            pc1[3] += pa[25] * pb[27];
            pc2[3] += pa[26] * pb[27];
            pc3[3] += pa[27] * pb[27];

            pc0[0] += pa[28] * pb[28];
            pc1[0] += pa[29] * pb[28];
            pc2[0] += pa[30] * pb[28];
            pc3[0] += pa[31] * pb[28];
            pc0[1] += pa[28] * pb[29];
            pc1[1] += pa[29] * pb[29];
            pc2[1] += pa[30] * pb[29];
            pc3[1] += pa[31] * pb[29];
            pc0[2] += pa[28] * pb[30];
            pc1[2] += pa[29] * pb[30];
            pc2[2] += pa[30] * pb[30];
            pc3[2] += pa[31] * pb[30];
            pc0[3] += pa[28] * pb[31];
            pc1[3] += pa[29] * pb[31];
            pc2[3] += pa[30] * pb[31];
            pc3[3] += pa[31] * pb[31];

            pa += 32;
            pb += 32;
        }
        if (j + 3 < k) {
            j += 4;
            pc0[0] += pa[0] * pb[0];
            pc1[0] += pa[1] * pb[0];
            pc2[0] += pa[2] * pb[0];
            pc3[0] += pa[3] * pb[0];
            pc0[1] += pa[0] * pb[1];
            pc1[1] += pa[1] * pb[1];
            pc2[1] += pa[2] * pb[1];
            pc3[1] += pa[3] * pb[1];
            pc0[2] += pa[0] * pb[2];
            pc1[2] += pa[1] * pb[2];
            pc2[2] += pa[2] * pb[2];
            pc3[2] += pa[3] * pb[2];
            pc0[3] += pa[0] * pb[3];
            pc1[3] += pa[1] * pb[3];
            pc2[3] += pa[2] * pb[3];
            pc3[3] += pa[3] * pb[3];

            pc0[0] += pa[4] * pb[4];
            pc1[0] += pa[5] * pb[4];
            pc2[0] += pa[6] * pb[4];
            pc3[0] += pa[7] * pb[4];
            pc0[1] += pa[4] * pb[5];
            pc1[1] += pa[5] * pb[5];
            pc2[1] += pa[6] * pb[5];
            pc3[1] += pa[7] * pb[5];
            pc0[2] += pa[4] * pb[6];
            pc1[2] += pa[5] * pb[6];
            pc2[2] += pa[6] * pb[6];
            pc3[2] += pa[7] * pb[6];
            pc0[3] += pa[4] * pb[7];
            pc1[3] += pa[5] * pb[7];
            pc2[3] += pa[6] * pb[7];
            pc3[3] += pa[7] * pb[7];

            pc0[0] += pa[8] * pb[8];
            pc1[0] += pa[9] * pb[8];
            pc2[0] += pa[10] * pb[8];
            pc3[0] += pa[11] * pb[8];
            pc0[1] += pa[8] * pb[9];
            pc1[1] += pa[9] * pb[9];
            pc2[1] += pa[10] * pb[9];
            pc3[1] += pa[11] * pb[9];
            pc0[2] += pa[8] * pb[10];
            pc1[2] += pa[9] * pb[10];
            pc2[2] += pa[10] * pb[10];
            pc3[2] += pa[11] * pb[10];
            pc0[3] += pa[8] * pb[11];
            pc1[3] += pa[9] * pb[11];
            pc2[3] += pa[10] * pb[11];
            pc3[3] += pa[11] * pb[11];

            pc0[0] += pa[12] * pb[12];
            pc1[0] += pa[13] * pb[12];
            pc2[0] += pa[14] * pb[12];
            pc3[0] += pa[15] * pb[12];
            pc0[1] += pa[12] * pb[13];
            pc1[1] += pa[13] * pb[13];
            pc2[1] += pa[14] * pb[13];
            pc3[1] += pa[15] * pb[13];
            pc0[2] += pa[12] * pb[14];
            pc1[2] += pa[13] * pb[14];
            pc2[2] += pa[14] * pb[14];
            pc3[2] += pa[15] * pb[14];
            pc0[3] += pa[12] * pb[15];
            pc1[3] += pa[13] * pb[15];
            pc2[3] += pa[14] * pb[15];
            pc3[3] += pa[15] * pb[15];

            pa += 16;
            pb += 16;
        }
        if (j + 1 < k) {
            j += 2;
            pc0[0] += pa[0] * pb[0];
            pc1[0] += pa[1] * pb[0];
            pc2[0] += pa[2] * pb[0];
            pc3[0] += pa[3] * pb[0];
            pc0[1] += pa[0] * pb[1];
            pc1[1] += pa[1] * pb[1];
            pc2[1] += pa[2] * pb[1];
            pc3[1] += pa[3] * pb[1];
            pc0[2] += pa[0] * pb[2];
            pc1[2] += pa[1] * pb[2];
            pc2[2] += pa[2] * pb[2];
            pc3[2] += pa[3] * pb[2];
            pc0[3] += pa[0] * pb[3];
            pc1[3] += pa[1] * pb[3];
            pc2[3] += pa[2] * pb[3];
            pc3[3] += pa[3] * pb[3];

            pc0[0] += pa[4] * pb[4];
            pc1[0] += pa[5] * pb[4];
            pc2[0] += pa[6] * pb[4];
            pc3[0] += pa[7] * pb[4];
            pc0[1] += pa[4] * pb[5];
            pc1[1] += pa[5] * pb[5];
            pc2[1] += pa[6] * pb[5];
            pc3[1] += pa[7] * pb[5];
            pc0[2] += pa[4] * pb[6];
            pc1[2] += pa[5] * pb[6];
            pc2[2] += pa[6] * pb[6];
            pc3[2] += pa[7] * pb[6];
            pc0[3] += pa[4] * pb[7];
            pc1[3] += pa[5] * pb[7];
            pc2[3] += pa[6] * pb[7];
            pc3[3] += pa[7] * pb[7];

            pa += 8;
            pb += 8;
        }
        if (j < k) {
            pc0[0] += pa[0] * pb[0];
            pc1[0] += pa[1] * pb[0];
            pc2[0] += pa[2] * pb[0];
            pc3[0] += pa[3] * pb[0];
            pc0[1] += pa[0] * pb[1];
            pc1[1] += pa[1] * pb[1];
            pc2[1] += pa[2] * pb[1];
            pc3[1] += pa[3] * pb[1];
            pc0[2] += pa[0] * pb[2];
            pc1[2] += pa[1] * pb[2];
            pc2[2] += pa[2] * pb[2];
            pc3[2] += pa[3] * pb[2];
            pc0[3] += pa[0] * pb[3];
            pc1[3] += pa[1] * pb[3];
            pc2[3] += pa[2] * pb[3];
            pc3[3] += pa[3] * pb[3];

            pa += 4;
            pb += 4;
        }
        if (fuse_relu) {
            pc0[0] = pc0[0] > 0 ? pc0[0] : 0;
            pc0[1] = pc0[1] > 0 ? pc0[1] : 0;
            pc0[2] = pc0[2] > 0 ? pc0[2] : 0;
            pc0[3] = pc0[3] > 0 ? pc0[3] : 0;

            pc1[0] = pc1[0] > 0 ? pc1[0] : 0;
            pc1[1] = pc1[1] > 0 ? pc1[1] : 0;
            pc1[2] = pc1[2] > 0 ? pc1[2] : 0;
            pc1[3] = pc1[3] > 0 ? pc1[3] : 0;

            pc2[0] = pc2[0] > 0 ? pc2[0] : 0;
            pc2[1] = pc2[1] > 0 ? pc2[1] : 0;
            pc2[2] = pc2[2] > 0 ? pc2[2] : 0;
            pc2[3] = pc2[3] > 0 ? pc2[3] : 0;

            pc3[0] = pc3[0] > 0 ? pc3[0] : 0;
            pc3[1] = pc3[1] > 0 ? pc3[1] : 0;
            pc3[2] = pc3[2] > 0 ? pc3[2] : 0;
            pc3[3] = pc3[3] > 0 ? pc3[3] : 0;
        }
        pc0 += 4;
        pc1 += 4;
        pc2 += 4;
        pc3 += 4;
    }
    if (n2 > 0) {
        pa = sa;
        pc0[0] = pc0[1] = *bias;
        pc1[0] = pc1[1] = *(bias + 1);
        pc2[0] = pc2[1] = *(bias + 2);
        pc3[0] = pc3[1] = *(bias + 3);
        float *pb0 = pb;
        float *pb1 = pb0 + k;
        int j = 0;
        for (; j + 7 < k; j += 8) {
            pc0[0] += pa[0] * pb0[0];
            pc1[0] += pa[1] * pb0[0];
            pc2[0] += pa[2] * pb0[0];
            pc3[0] += pa[3] * pb0[0];
            pc0[1] += pa[0] * pb1[0];
            pc1[1] += pa[1] * pb1[0];
            pc2[1] += pa[2] * pb1[0];
            pc3[1] += pa[3] * pb1[0];

            pc0[0] += pa[4] * pb0[1];
            pc1[0] += pa[5] * pb0[1];
            pc2[0] += pa[6] * pb0[1];
            pc3[0] += pa[7] * pb0[1];
            pc0[1] += pa[4] * pb1[1];
            pc1[1] += pa[5] * pb1[1];
            pc2[1] += pa[6] * pb1[1];
            pc3[1] += pa[7] * pb1[1];

            pc0[0] += pa[8] * pb0[2];
            pc1[0] += pa[9] * pb0[2];
            pc2[0] += pa[10] * pb0[2];
            pc3[0] += pa[11] * pb0[2];
            pc0[1] += pa[8] * pb1[2];
            pc1[1] += pa[9] * pb1[2];
            pc2[1] += pa[10] * pb1[2];
            pc3[1] += pa[11] * pb1[2];

            pc0[0] += pa[12] * pb0[3];
            pc1[0] += pa[13] * pb0[3];
            pc2[0] += pa[14] * pb0[3];
            pc3[0] += pa[15] * pb0[3];
            pc0[1] += pa[12] * pb1[3];
            pc1[1] += pa[13] * pb1[3];
            pc2[1] += pa[14] * pb1[3];
            pc3[1] += pa[15] * pb1[3];

            pc0[0] += pa[16] * pb0[4];
            pc1[0] += pa[17] * pb0[4];
            pc2[0] += pa[18] * pb0[4];
            pc3[0] += pa[19] * pb0[4];
            pc0[1] += pa[16] * pb1[4];
            pc1[1] += pa[17] * pb1[4];
            pc2[1] += pa[18] * pb1[4];
            pc3[1] += pa[19] * pb1[4];

            pc0[0] += pa[20] * pb0[5];
            pc1[0] += pa[21] * pb0[5];
            pc2[0] += pa[22] * pb0[5];
            pc3[0] += pa[23] * pb0[5];
            pc0[1] += pa[20] * pb1[5];
            pc1[1] += pa[21] * pb1[5];
            pc2[1] += pa[22] * pb1[5];
            pc3[1] += pa[23] * pb1[5];

            pc0[0] += pa[24] * pb0[6];
            pc1[0] += pa[25] * pb0[6];
            pc2[0] += pa[26] * pb0[6];
            pc3[0] += pa[27] * pb0[6];
            pc0[1] += pa[24] * pb1[6];
            pc1[1] += pa[25] * pb1[6];
            pc2[1] += pa[26] * pb1[6];
            pc3[1] += pa[27] * pb1[6];

            pc0[0] += pa[28] * pb0[7];
            pc1[0] += pa[29] * pb0[7];
            pc2[0] += pa[30] * pb0[7];
            pc3[0] += pa[31] * pb0[7];
            pc0[1] += pa[28] * pb1[7];
            pc1[1] += pa[29] * pb1[7];
            pc2[1] += pa[30] * pb1[7];
            pc3[1] += pa[31] * pb1[7];

            pa += 32;
            pb0 += 8;
            pb1 += 8;
        }
        if (j + 3 < k) {
            j += 4;
            pc0[0] += pa[0] * pb0[0];
            pc1[0] += pa[1] * pb0[0];
            pc2[0] += pa[2] * pb0[0];
            pc3[0] += pa[3] * pb0[0];
            pc0[1] += pa[0] * pb1[0];
            pc1[1] += pa[1] * pb1[0];
            pc2[1] += pa[2] * pb1[0];
            pc3[1] += pa[3] * pb1[0];

            pc0[0] += pa[4] * pb0[1];
            pc1[0] += pa[5] * pb0[1];
            pc2[0] += pa[6] * pb0[1];
            pc3[0] += pa[7] * pb0[1];
            pc0[1] += pa[4] * pb1[1];
            pc1[1] += pa[5] * pb1[1];
            pc2[1] += pa[6] * pb1[1];
            pc3[1] += pa[7] * pb1[1];

            pc0[0] += pa[8] * pb0[2];
            pc1[0] += pa[9] * pb0[2];
            pc2[0] += pa[10] * pb0[2];
            pc3[0] += pa[11] * pb0[2];
            pc0[1] += pa[8] * pb1[2];
            pc1[1] += pa[9] * pb1[2];
            pc2[1] += pa[10] * pb1[2];
            pc3[1] += pa[11] * pb1[2];

            pc0[0] += pa[12] * pb0[3];
            pc1[0] += pa[13] * pb0[3];
            pc2[0] += pa[14] * pb0[3];
            pc3[0] += pa[15] * pb0[3];
            pc0[1] += pa[12] * pb1[3];
            pc1[1] += pa[13] * pb1[3];
            pc2[1] += pa[14] * pb1[3];
            pc3[1] += pa[15] * pb1[3];

            pa += 16;
            pb0 += 4;
            pb1 += 4;
        }
        if (j + 1 < k) {
            j += 2;
            pc0[0] += pa[0] * pb0[0];
            pc1[0] += pa[1] * pb0[0];
            pc2[0] += pa[2] * pb0[0];
            pc3[0] += pa[3] * pb0[0];
            pc0[1] += pa[0] * pb1[0];
            pc1[1] += pa[1] * pb1[0];
            pc2[1] += pa[2] * pb1[0];
            pc3[1] += pa[3] * pb1[0];

            pc0[0] += pa[4] * pb0[1];
            pc1[0] += pa[5] * pb0[1];
            pc2[0] += pa[6] * pb0[1];
            pc3[0] += pa[7] * pb0[1];
            pc0[1] += pa[4] * pb1[1];
            pc1[1] += pa[5] * pb1[1];
            pc2[1] += pa[6] * pb1[1];
            pc3[1] += pa[7] * pb1[1];

            pa += 8;
            pb0 += 2;
            pb1 += 2;
        }
        if (j < k) {
            pc0[0] += pa[0] * pb0[0];
            pc1[0] += pa[1] * pb0[0];
            pc2[0] += pa[2] * pb0[0];
            pc3[0] += pa[3] * pb0[0];
            pc0[1] += pa[0] * pb1[0];
            pc1[1] += pa[1] * pb1[0];
            pc2[1] += pa[2] * pb1[0];
            pc3[1] += pa[3] * pb1[0];

            pa += 4;
            pb0 += 1;
            pb1 += 1;
        }
        if (fuse_relu) {
            pc0[0] = pc0[0] > 0 ? pc0[0] : 0;
            pc0[1] = pc0[1] > 0 ? pc0[1] : 0;

            pc1[0] = pc1[0] > 0 ? pc1[0] : 0;
            pc1[1] = pc1[1] > 0 ? pc1[1] : 0;

            pc2[0] = pc2[0] > 0 ? pc2[0] : 0;
            pc2[1] = pc2[1] > 0 ? pc2[1] : 0;

            pc3[0] = pc3[0] > 0 ? pc3[0] : 0;
            pc3[1] = pc3[1] > 0 ? pc3[1] : 0;
        }
        pc0 += 2;
        pc1 += 2;
        pc2 += 2;
        pc3 += 2;
        pb += 2 * k;
    }
    if (n1 > 0) {
        pa = sa;
        pc0[0] = *bias;
        pc1[0] = *(bias + 1);
        pc2[0] = *(bias + 2);
        pc3[0] = *(bias + 3);
        int j = 0;
        for (; j + 7 < k; j += 8) {
            pc0[0] += pa[0] * pb[0];
            pc1[0] += pa[1] * pb[0];
            pc2[0] += pa[2] * pb[0];
            pc3[0] += pa[3] * pb[0];

            pc0[0] += pa[4] * pb[1];
            pc1[0] += pa[5] * pb[1];
            pc2[0] += pa[6] * pb[1];
            pc3[0] += pa[7] * pb[1];

            pc0[0] += pa[8] * pb[2];
            pc1[0] += pa[9] * pb[2];
            pc2[0] += pa[10] * pb[2];
            pc3[0] += pa[11] * pb[2];

            pc0[0] += pa[12] * pb[3];
            pc1[0] += pa[13] * pb[3];
            pc2[0] += pa[14] * pb[3];
            pc3[0] += pa[15] * pb[3];

            pc0[0] += pa[16] * pb[4];
            pc1[0] += pa[17] * pb[4];
            pc2[0] += pa[18] * pb[4];
            pc3[0] += pa[19] * pb[4];

            pc0[0] += pa[20] * pb[5];
            pc1[0] += pa[21] * pb[5];
            pc2[0] += pa[22] * pb[5];
            pc3[0] += pa[23] * pb[5];

            pc0[0] += pa[24] * pb[6];
            pc1[0] += pa[25] * pb[6];
            pc2[0] += pa[26] * pb[6];
            pc3[0] += pa[27] * pb[6];

            pc0[0] += pa[28] * pb[7];
            pc1[0] += pa[29] * pb[7];
            pc2[0] += pa[30] * pb[7];
            pc3[0] += pa[31] * pb[7];

            pa += 32;
            pb += 8;
        }
        if (j + 3 < k) {
            j += 4;
            pc0[0] += pa[0] * pb[0];
            pc1[0] += pa[1] * pb[0];
            pc2[0] += pa[2] * pb[0];
            pc3[0] += pa[3] * pb[0];

            pc0[0] += pa[4] * pb[1];
            pc1[0] += pa[5] * pb[1];
            pc2[0] += pa[6] * pb[1];
            pc3[0] += pa[7] * pb[1];

            pc0[0] += pa[8] * pb[2];
            pc1[0] += pa[9] * pb[2];
            pc2[0] += pa[10] * pb[2];
            pc3[0] += pa[11] * pb[2];

            pc0[0] += pa[12] * pb[3];
            pc1[0] += pa[13] * pb[3];
            pc2[0] += pa[14] * pb[3];
            pc3[0] += pa[15] * pb[3];

            pa += 16;
            pb += 4;
        }
        if (j + 1 < k) {
            j += 2;
            pc0[0] += pa[0] * pb[0];
            pc1[0] += pa[1] * pb[0];
            pc2[0] += pa[2] * pb[0];
            pc3[0] += pa[3] * pb[0];

            pc0[0] += pa[4] * pb[1];
            pc1[0] += pa[5] * pb[1];
            pc2[0] += pa[6] * pb[1];
            pc3[0] += pa[7] * pb[1];

            pa += 8;
            pb += 2;
        }
        if (j < k) {
            pc0[0] += pa[0] * pb[0];
            pc1[0] += pa[1] * pb[0];
            pc2[0] += pa[2] * pb[0];
            pc3[0] += pa[3] * pb[0];

            pa += 4;
            pb += 1;
        }
        if (fuse_relu) {
            pc0[0] = pc0[0] > 0 ? pc0[0] : 0;

            pc1[0] = pc1[0] > 0 ? pc1[0] : 0;

            pc2[0] = pc2[0] > 0 ? pc2[0] : 0;

            pc3[0] = pc3[0] > 0 ? pc3[0] : 0;
        }
        pc0 += 1;
        pc1 += 1;
        pc2 += 1;
        pc3 += 1;
    }
#endif  // __riscv_vector
}

static inline void kernel_m4_f32_1(float *dst, float *sa, float *sb, int m, int k, int n, int ldc,
                                   float *bias, bool fuse_relu)
{
    asm volatile(
        "vsetvli        zero, zero, e32, m1\n\t"  // set vl = 4

        "flw            fs0, 0(%2)\n\t"
        "flw            fs1, 4(%2)\n\t"
        "flw            fs2, 8(%2)\n\t"
        "flw            fs3, 12(%2)\n\t"

        // init output addr
        "slli           t5, %6, 2\n\t"  // t5_tmp = ldx * 4
        "mv             a0, %3\n\t"
        "add            a1, a0, t5\n\t"
        "add            a2, a1, t5\n\t"
        "add            a3, a2, t5\n\t"

        "srai           t0, %5, 2\n\t"  // t0 = n >> 2 (n4)
        "beqz           t0, 4f\n\t"

        "1:\n\t"  // m4n4
                  // start kernel_m4n4
        "vfmv.v.f       v24, fs0\n\t"
        "vfmv.v.f       v25, fs1\n\t"
        "vfmv.v.f       v26, fs2\n\t"
        "vfmv.v.f       v27, fs3\n\t"  // init acc = bias

        "mv             t6, %0\n\t"  // t6 hold kernel 4 lines start addr
        "mv             t5, %4\n\t"  // t5 = k (k > 0)

        "2:\n\t"
        // start subkernel_m4n4k1
        "vle.v          v1, (%1)\n\t"
        "addi           %1, %1, 16\n\t"
        "flw            fa0, 0(t6)\n\t"
        "flw            fa1, 4(t6)\n\t"
        "flw            fa2, 8(t6)\n\t"
        "flw            fa3, 12(t6)\n\t"
        "addi           t6, t6, 16\n\t"

        "vfmacc.vf      v24, fa0, v1\n\t"
        "vfmacc.vf      v25, fa1, v1\n\t"
        "vfmacc.vf      v26, fa2, v1\n\t"
        "vfmacc.vf      v27, fa3, v1\n\t"

        "addi           t5, t5, -1\n\t"
        "bnez           t5, 2b\n\t"

        "3:\n\t"  // end kernel_m4n4

        "vse.v          v24, (a0)\n\t"
        "addi           a0, a0, 16\n\t"
        "vse.v          v25, (a1)\n\t"
        "addi           a1, a1, 16\n\t"
        "vse.v          v26, (a2)\n\t"
        "addi           a2, a2, 16\n\t"
        "vse.v          v27, (a3)\n\t"
        "addi           a3, a3, 16\n\t"

        "addi           t0, t0, -1\n\t"
        "bnez           t0, 1b\n\t"

        "4:\n\t"                        // m4n2
        "andi           t0, %5, 3\n\t"  // n & 3
        "srai           t0, t0, 1\n\t"  // (n & 3) >> 2
        "beqz           t0, 7f\n\t"     // jump to m4n1
        // start kernel_m4n2
        "vle.v          v24, (%2)\n\t"
        "vle.v          v25, (%2)\n\t"  // init acc = bias

        // init addr for pa, pb and pc
        "slli           t0, %4, 2\n\t"  // t0_tmp = k * 4

        "mv             t6, %0\n\t"  // t6 hold pa(kernel) 2 lines start addr

        "mv             a4, %1\n\t"
        "add            a5, a4, t0\n\t"  // a4-a5 hold pb(input) 2 cols addr

        "addi           a1, a0, 4\n\t"  // a0-a1 hold pc(output) addr

        "mv             t5, %4\n\t"  // t5 = k

        "5:\n\t"
        // start subkernel_m4n2k1
        "vle.v          v1, (t6)\n\t"
        "addi           t6, t6, 16\n\t"
        "flw            fa0, 0(a4)\n\t"
        "vfmacc.vf      v24, fa0, v1\n\t"
        "flw            fa1, 0(a5)\n\t"
        "vfmacc.vf      v25, fa1, v1\n\t"

        "addi           a4, a4, 4\n\t"
        "addi           a5, a5, 4\n\t"

        "addi           t5, t5, -1\n\t"
        "bnez           t5, 5b\n\t"

        "6:\n\t"                        // end kernel_m4n2
        "slli           t0, %6, 2\n\t"  // t0_tmp = ldx * 4 (store_stride)

        "vsse.v         v24, (a0), t0\n\t"
        "vsse.v         v25, (a1), t0\n\t"

        "addi           a0, a0, 8\n\t"   // updata output start addr ( +2 cols)
        "slli           t0, %4, 3\n\t"   // t_tmp = k * 2 * 4
        "add            %1, %1, t0\n\t"  // updata pb start addr

        "7:\n\t"                        // m4n1
        "andi           t0, %5, 1\n\t"  // n & 1
        "beqz           t0, 10f\n\t"    // jump to ending
        // start kernel_m8n1

        "vle.v          v24, (%2)\n\t"  // init out_tmp = bias

        // init addr for pa, pb and pc
        "mv             t6, %0\n\t"  // t6 hold pa(kernel) 8 lines start addr
        "mv             a4, %1\n\t"  // a4 hold pb(input) 1 cols addr
                                     // a0 hold pc(output) addr

        "mv             t5, %4\n\t"  // t5 = k

        "8:\n\t"
        // start subkernel_m8n1k8
        "vle.v          v1, (t6)\n\t"
        "addi           t6, t6, 16\n\t"
        "flw            fa0, 0(a4)\n\t"
        "vfmacc.vf      v24, fa0, v1\n\t"  // 0

        "addi           a4, a4, 4\n\t"

        "addi           t5, t5, -1\n\t"
        "bnez           t5, 8b\n\t"

        "9:\n\t"                        // end kernel_m8n1
        "slli           t0, %6, 2\n\t"  // t0_tmp = ldx * 4 (store_stride)

        "vsse.v         v24, (a0), t0\n\t"

        "10:\n\t"  // ending

        : "=r"(sa),    // %0
          "=r"(sb),    // %1
          "=r"(bias),  // %2
          "=r"(dst),   // %3
          "=r"(k),     // %4
          "=r"(n),     // %5
          "=r"(ldc)    // %6
        : "0"(sa), "1"(sb), "2"(bias), "3"(dst), "4"(k), "5"(n), "6"(ldc)
        : "v1", "v24", "v25", "v26", "v27", "v28", "v29", "v30", "v31", "a0", "a1", "a2", "a3",
          "a4", "a5", "a6", "a7", "t0", "t5", "t6", "fa0", "fa1", "fa2", "fa3", "fa4", "fa5", "fa6",
          "fa7", "fs0", "fs1", "fs2", "fs3", "fs4", "fs5", "fs6", "fs7");
}

void shl_c906_sgemm_kernel_f32(float *dst, const float *sa, const float *sb, int m, int k, int n,
                               int ldc, float *bias, bool fuse_relu)
{
    float *pa = (float *)sa;
    float *pb = (float *)sb;
    float *pc = dst;

    bool flag_bias = 1;  // default: conv2d layer include bias
    if (bias == NULL) {
        flag_bias = 0;
        bias = (float *)shl_mem_alloc(m * 4);
    }
    float *bias_tmp = bias;

    const int mm = (m >> 2) << 2;

    for (int i = 0; i < mm; i += 4) {
        kernel_m4_f32_1(pc + i * ldc, pa + i * k, pb, m, k, n, ldc, bias_tmp + i, fuse_relu);
    }

    pa += mm * k;
    pc += mm * ldc;
    bias_tmp += mm;

    switch (m - mm) {
        case 3:
            kernel_m2_f32(pc, pa, pb, m, k, n, ldc, bias_tmp, fuse_relu);
            pc += 2 * ldc;
            pa += 2 * k;
            bias_tmp += 2;
            kernel_m1_f32(pc, pa, pb, m, k, n, ldc, bias_tmp, fuse_relu);
            break;
        case 2:
            kernel_m2_f32(pc, pa, pb, m, k, n, ldc, bias_tmp, fuse_relu);
            break;
        case 1:
            kernel_m1_f32(pc, pa, pb, m, k, n, ldc, bias_tmp, fuse_relu);
            break;
        case 0:
            break;
        default:
            break;
    }
    if (!flag_bias) {
        shl_mem_free(bias);
        bias = NULL;
    }
}
